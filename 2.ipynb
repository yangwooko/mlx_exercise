{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÌóàÍπÖ ÌéòÏù¥Ïä§ Î™®Îç∏ÏùÑ ÏñëÏûêÌôî ÌïòÍ≥† ÏÉàÎ°úÏö¥ ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Ïñ∏Ïñ¥Î•º ÌïôÏäµ ÏãúÌÇ§Îäî ÏÉòÌîå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q mlx_lm\n",
    "!pip install -q jinja2\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î™®Îç∏ ÏñëÏûêÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[INFO] Loading\n",
      "Fetching 10 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 188932.61it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "# !python ./mlx-examples/lora/convert.py --hf-path microsoft/phi-2 --mlx-path ./my_models/mlx_phi2 -q\n",
    "# !python ./mlx-examples/lora/convert.py --hf-path daekeun-ml/phi-2-ko-v0.1 --mlx-path ./my_models/mlx_phi2_ko -q\n",
    "!python ./mlx-examples/lora/convert.py --hf-path microsoft/phi-2 --mlx-path ./my_models/mlx_phi2 -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î™®Îç∏ ÎèôÏûë ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './my_models/mlx_phi2'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlx_lm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, generate\n\u001b[0;32m----> 3\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./my_models/mlx_phi2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/mlx-examples-MmUb6Hoy/lib/python3.11/site-packages/mlx_lm/utils.py:282\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path_or_hf_repo, tokenizer_config, adapter_file)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    264\u001b[0m     path_or_hf_repo: \u001b[38;5;28mstr\u001b[39m, tokenizer_config\u001b[38;5;241m=\u001b[39m{}, adapter_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[nn\u001b[38;5;241m.\u001b[39mModule, PreTrainedTokenizer]:\n\u001b[1;32m    266\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    Load the model and tokenizer from a given path or a huggingface repository.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m        ValueError: If model class or args class are not found.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_hf_repo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(model_path)\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/mlx-examples-MmUb6Hoy/lib/python3.11/site-packages/mlx_lm/utils.py:75\u001b[0m, in \u001b[0;36mget_model_path\u001b[0;34m(path_or_hf_repo)\u001b[0m\n\u001b[1;32m     72\u001b[0m model_path \u001b[38;5;241m=\u001b[39m Path(path_or_hf_repo)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     74\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m Path(\n\u001b[0;32m---> 75\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_hf_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.tiktoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_path\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/mlx-examples-MmUb6Hoy/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/mlx-examples-MmUb6Hoy/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './my_models/mlx_phi2'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_phi2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the python programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "# This is a joke\n",
      "# What do you call a fish that wears a bowtie?\n",
      "# Sofishticated.\n",
      "\n",
      "==========\n",
      "Prompt: 73.874 tokens-per-sec\n",
      "Generation: 30.769 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in the OPL programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "// This program prints a joke using comments in the OPL programming language\n",
      "\n",
      "int main() {\n",
      "    // Declare a variable to store the joke\n",
      "    string joke;\n",
      "    // Set the value of the joke\n",
      "    joke = \"Why did the programmer quit his job? Because he couldn't take the heat!\";\n",
      "    // Print the joke\n",
      "    cout\n",
      "==========\n",
      "Prompt: 1.913 tokens-per-sec\n",
      "Generation: 3.359 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks in the OPL programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "Here's a joke using remarks in the OPL programming language:\n",
      "\n",
      "```\n",
      "int x = 10;\n",
      "int y = 5;\n",
      "int z = x + y;\n",
      "\n",
      "// This is a comment\n",
      "// It explains what the code does\n",
      "// It is ignored by the compiler\n",
      "\n",
      "// This is a remark\n",
      "// It is a special comment\n",
      "// It can be used to add extra information\n",
      "// It is\n",
      "==========\n",
      "Prompt: 34.654 tokens-per-sec\n",
      "Generation: 3.012 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Write a program that checks if a given year, is a leap year in the OPL programming language\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "```python\n",
      "# Solution\n",
      "year = int(input(\"Enter a year: \"))\n",
      "\n",
      "if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "        if year % 400 == 0:\n",
      "            print(year, \"is a leap year\")\n",
      "        else:\n",
      "            print(year, \"is not a leap year\")\n",
      "    else:\n",
      "        print(year, \"is a leap year\")\n",
      "else:\n",
      "    print\n",
      "==========\n",
      "Prompt: 25.556 tokens-per-sec\n",
      "Generation: 3.206 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨ÏÑ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPL Programming Language ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Í∞ÄÏ†∏Ïò®Îã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4497ebc72e844dbbcae1fbc316c2abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/36.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.0k/14.0k [00:00<00:00, 49.6kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cbb75f140f4f8e8a98ad7c979bb1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('chrishayuk/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['<s>[INST] What does OPL stand for in the OPL programming language? [/INST] OPL is short for Open Programming Language </s>',\n",
       "  '<s>[INST] Which company developed the OPL programmung language? [/INST] Psion Ltd created OPL for the Psion Organiser </s>',\n",
       "  '<s>[INST] Which was the original name for the OPL programming language? [/INST] The OPL language was originally named Organiser Programming Language </s>']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    q, a = example['text'].replace('<s>[INST]', '').replace('</s>', '').split('[/INST]')\n",
    "    return {'text': f\"### Question: {q}\\n ### Answer: {a}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4739486a65404cca949a74204c9bc02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "converted_dataset = dataset['train'].map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data(dataset, valid_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    valid_indices = indices[:int(len(dataset) * valid_ratio)]\n",
    "    train_indices = indices[int(len(dataset) * valid_ratio):]\n",
    "    \n",
    "    return dataset.select(train_indices), dataset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 43\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data(converted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a299920f3342f280579a150436703a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11005"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/train.jsonl'\n",
    "\n",
    "train_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa7dbf981344a9dba6cba1b55d7c1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3346"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/valid.jsonl'\n",
    "\n",
    "valid_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î™®Îç∏ ÌååÏù∏ÌäúÎãù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Total parameters 547.036M\n",
      "Trainable parameters 1.311M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.440, Val took 5.352s\n",
      "Iter 10: Train loss 2.360, It/sec 0.268, Tokens/sec 107.435\n",
      "Iter 20: Train loss 2.035, It/sec 0.255, Tokens/sec 91.322\n",
      "Iter 30: Train loss 1.766, It/sec 0.274, Tokens/sec 95.226\n",
      "Iter 40: Train loss 1.360, It/sec 0.254, Tokens/sec 93.984\n",
      "Iter 50: Train loss 1.151, It/sec 0.259, Tokens/sec 90.740\n",
      "Iter 60: Train loss 1.044, It/sec 0.254, Tokens/sec 94.548\n",
      "Iter 70: Train loss 0.946, It/sec 0.300, Tokens/sec 107.431\n",
      "Iter 80: Train loss 0.854, It/sec 0.264, Tokens/sec 95.902\n",
      "Iter 90: Train loss 0.726, It/sec 0.259, Tokens/sec 94.252\n",
      "Iter 100: Train loss 0.660, It/sec 0.237, Tokens/sec 91.137\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 0.592, It/sec 0.279, Tokens/sec 93.338\n",
      "Iter 120: Train loss 0.567, It/sec 0.264, Tokens/sec 92.269\n",
      "Iter 130: Train loss 0.451, It/sec 0.276, Tokens/sec 95.866\n",
      "Iter 140: Train loss 0.423, It/sec 0.252, Tokens/sec 92.441\n",
      "Iter 150: Train loss 0.423, It/sec 0.263, Tokens/sec 98.005\n",
      "Iter 160: Train loss 0.316, It/sec 0.240, Tokens/sec 89.218\n",
      "Iter 170: Train loss 0.298, It/sec 0.251, Tokens/sec 93.612\n",
      "Iter 180: Train loss 0.251, It/sec 0.241, Tokens/sec 89.994\n",
      "Iter 190: Train loss 0.210, It/sec 0.300, Tokens/sec 102.005\n",
      "Iter 200: Train loss 0.190, It/sec 0.240, Tokens/sec 91.932\n",
      "Iter 200: Val loss 1.473, Val took 5.200s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 0.173, It/sec 0.256, Tokens/sec 90.295\n",
      "Iter 220: Train loss 0.138, It/sec 0.282, Tokens/sec 96.385\n",
      "Iter 230: Train loss 0.132, It/sec 0.262, Tokens/sec 94.655\n",
      "Iter 240: Train loss 0.118, It/sec 0.234, Tokens/sec 86.467\n",
      "Iter 250: Train loss 0.105, It/sec 0.245, Tokens/sec 94.337\n",
      "Iter 260: Train loss 0.112, It/sec 0.280, Tokens/sec 96.009\n",
      "Iter 270: Train loss 0.095, It/sec 0.234, Tokens/sec 86.655\n",
      "Iter 280: Train loss 0.093, It/sec 0.260, Tokens/sec 90.889\n",
      "Iter 290: Train loss 0.087, It/sec 0.241, Tokens/sec 93.871\n",
      "Iter 300: Train loss 0.091, It/sec 0.240, Tokens/sec 84.628\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 0.085, It/sec 0.266, Tokens/sec 90.436\n",
      "Iter 320: Train loss 0.079, It/sec 0.289, Tokens/sec 101.535\n",
      "Iter 330: Train loss 0.075, It/sec 0.245, Tokens/sec 86.772\n",
      "Iter 340: Train loss 0.077, It/sec 0.301, Tokens/sec 103.637\n",
      "Iter 350: Train loss 0.071, It/sec 0.237, Tokens/sec 85.359\n",
      "Iter 360: Train loss 0.073, It/sec 0.260, Tokens/sec 88.948\n",
      "Iter 370: Train loss 0.068, It/sec 0.264, Tokens/sec 99.604\n",
      "Iter 380: Train loss 0.074, It/sec 0.278, Tokens/sec 99.073\n",
      "Iter 390: Train loss 0.061, It/sec 0.258, Tokens/sec 97.088\n",
      "Iter 400: Train loss 0.066, It/sec 0.250, Tokens/sec 93.618\n",
      "Iter 400: Val loss 1.737, Val took 5.200s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 0.059, It/sec 0.239, Tokens/sec 89.396\n",
      "Iter 420: Train loss 0.059, It/sec 0.246, Tokens/sec 91.350\n",
      "Iter 430: Train loss 0.063, It/sec 0.277, Tokens/sec 93.333\n",
      "Iter 440: Train loss 0.060, It/sec 0.250, Tokens/sec 90.673\n",
      "Iter 450: Train loss 0.059, It/sec 0.245, Tokens/sec 93.461\n",
      "Iter 460: Train loss 0.060, It/sec 0.289, Tokens/sec 97.903\n",
      "Iter 470: Train loss 0.055, It/sec 0.227, Tokens/sec 87.805\n",
      "Iter 480: Train loss 0.058, It/sec 0.220, Tokens/sec 80.733\n",
      "Iter 490: Train loss 0.063, It/sec 0.228, Tokens/sec 75.411\n",
      "Iter 500: Train loss 0.065, It/sec 0.278, Tokens/sec 98.441\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 0.054, It/sec 0.204, Tokens/sec 78.275\n",
      "Iter 520: Train loss 0.058, It/sec 0.236, Tokens/sec 84.618\n",
      "Iter 530: Train loss 0.058, It/sec 0.227, Tokens/sec 82.073\n",
      "Iter 540: Train loss 0.053, It/sec 0.215, Tokens/sec 82.081\n",
      "Iter 550: Train loss 0.055, It/sec 0.240, Tokens/sec 85.345\n",
      "Iter 560: Train loss 0.056, It/sec 0.202, Tokens/sec 76.042\n",
      "Iter 570: Train loss 0.055, It/sec 0.263, Tokens/sec 92.899\n",
      "Iter 580: Train loss 0.058, It/sec 0.254, Tokens/sec 89.189\n",
      "Iter 590: Train loss 0.054, It/sec 0.236, Tokens/sec 83.571\n",
      "Iter 600: Train loss 0.052, It/sec 0.195, Tokens/sec 71.642\n",
      "Iter 600: Val loss 1.821, Val took 5.452s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 0.058, It/sec 0.253, Tokens/sec 86.096\n",
      "Iter 620: Train loss 0.057, It/sec 0.209, Tokens/sec 75.334\n",
      "Iter 630: Train loss 0.053, It/sec 0.254, Tokens/sec 92.327\n",
      "Iter 640: Train loss 0.054, It/sec 0.196, Tokens/sec 71.466\n",
      "Iter 650: Train loss 0.050, It/sec 0.201, Tokens/sec 78.699\n",
      "Iter 660: Train loss 0.059, It/sec 0.284, Tokens/sec 93.913\n",
      "Iter 670: Train loss 0.055, It/sec 0.240, Tokens/sec 86.149\n",
      "Iter 680: Train loss 0.050, It/sec 0.216, Tokens/sec 82.662\n",
      "Iter 690: Train loss 0.055, It/sec 0.257, Tokens/sec 89.024\n",
      "Iter 700: Train loss 0.048, It/sec 0.224, Tokens/sec 84.491\n",
      "Iter 700: Saved adapter weights to adapters.npz.\n",
      "Iter 710: Train loss 0.051, It/sec 0.225, Tokens/sec 84.200\n",
      "Iter 720: Train loss 0.054, It/sec 0.259, Tokens/sec 92.603\n",
      "Iter 730: Train loss 0.053, It/sec 0.254, Tokens/sec 89.420\n",
      "Iter 740: Train loss 0.053, It/sec 0.239, Tokens/sec 94.418\n",
      "Iter 750: Train loss 0.055, It/sec 0.305, Tokens/sec 100.325\n",
      "Iter 760: Train loss 0.050, It/sec 0.244, Tokens/sec 95.724\n",
      "Iter 770: Train loss 0.054, It/sec 0.296, Tokens/sec 102.468\n",
      "Iter 780: Train loss 0.053, It/sec 0.287, Tokens/sec 93.993\n",
      "Iter 790: Train loss 0.053, It/sec 0.280, Tokens/sec 97.642\n",
      "Iter 800: Train loss 0.049, It/sec 0.232, Tokens/sec 89.592\n",
      "Iter 800: Val loss 1.860, Val took 5.194s\n",
      "Iter 800: Saved adapter weights to adapters.npz.\n",
      "Iter 810: Train loss 0.052, It/sec 0.258, Tokens/sec 92.055\n",
      "Iter 820: Train loss 0.048, It/sec 0.237, Tokens/sec 88.117\n",
      "Iter 830: Train loss 0.053, It/sec 0.279, Tokens/sec 95.520\n",
      "Iter 840: Train loss 0.050, It/sec 0.252, Tokens/sec 93.366\n",
      "Iter 850: Train loss 0.052, It/sec 0.243, Tokens/sec 87.925\n",
      "Iter 860: Train loss 0.051, It/sec 0.281, Tokens/sec 95.808\n",
      "Iter 870: Train loss 0.047, It/sec 0.221, Tokens/sec 88.505\n",
      "Iter 880: Train loss 0.051, It/sec 0.255, Tokens/sec 89.175\n",
      "Iter 890: Train loss 0.053, It/sec 0.253, Tokens/sec 89.551\n",
      "Iter 900: Train loss 0.052, It/sec 0.272, Tokens/sec 94.842\n",
      "Iter 900: Saved adapter weights to adapters.npz.\n",
      "Iter 910: Train loss 0.053, It/sec 0.275, Tokens/sec 94.379\n",
      "Iter 920: Train loss 0.049, It/sec 0.233, Tokens/sec 90.081\n",
      "Iter 930: Train loss 0.052, It/sec 0.250, Tokens/sec 89.847\n",
      "Iter 940: Train loss 0.051, It/sec 0.278, Tokens/sec 95.767\n",
      "Iter 950: Train loss 0.045, It/sec 0.225, Tokens/sec 90.487\n",
      "Iter 960: Train loss 0.050, It/sec 0.248, Tokens/sec 86.320\n",
      "Iter 970: Train loss 0.048, It/sec 0.240, Tokens/sec 89.070\n",
      "Iter 980: Train loss 0.052, It/sec 0.269, Tokens/sec 96.078\n",
      "Iter 990: Train loss 0.046, It/sec 0.227, Tokens/sec 90.633\n",
      "Iter 1000: Train loss 0.054, It/sec 0.287, Tokens/sec 94.576\n",
      "Iter 1000: Val loss 1.886, Val took 5.187s\n",
      "Iter 1000: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1\n",
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_phi2 --train --data ./my_datasets/chrishayuk/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(67883) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mv adapters.npz ./my_models/mlx_phi2_01.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(67884) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_phi2 --save-path ./my_models/mlx_phi2_ft_01 --adapter-file ./my_models/mlx_phi2_01.npz --hf-path microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÌååÏù∏ÌäúÎãù Í≤∞Í≥º ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_phi2_ft_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the python programming language\n",
      "### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite me a joke using comments only in the python programming language\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/mlx-examples-MmUb6Hoy/lib/python3.11/site-packages/mlx_lm/utils.py:167\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, prompt, temp, max_tokens, verbose, formatter)\u001b[0m\n\u001b[1;32m    165\u001b[0m     prompt_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m    166\u001b[0m     tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 167\u001b[0m tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    170\u001b[0m     s \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in the OPL programming language\n",
      ". \n",
      "ANSWER: // This is a joke in OPL\n",
      "PRINT \"Why did the chicken cross the road?\"\n",
      "PRINT \"To get to the other side.\"\n",
      "PRINT \"But it didn't want to get fried.\"\n",
      "\n",
      "==========\n",
      "Prompt: 90.347 tokens-per-sec\n",
      "Generation: 23.003 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks in the OPL programming language\n",
      ". \n",
      "ANSWER: Here's a joke using remarks in the OPL programming language:\n",
      "\n",
      "REM This is a joke in OPL\n",
      "PRINT \"Why did the programmer go to the doctor?\"\n",
      "READ \"Because he had a bug\"\n",
      "PRINT \"What did the doctor say?\"\n",
      "READ \"I'm not sure, but I'll check it out\"\n",
      "REM The joke is over\n",
      "END\n",
      "\n",
      "In this joke, the programmer goes to the doctor because he has a\n",
      "==========\n",
      "Prompt: 89.964 tokens-per-sec\n",
      "Generation: 19.317 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Write a program that checks if a given year, is a leap year in the OPL programming language\n",
      ".\n",
      "\n",
      "Question: Write a leap year checker program in OPL.\n",
      "\n",
      "Solution:\n",
      "To check if a year is a leap year, we need to check if it is divisible by 4 and not divisible by 100, or if it is divisible by 400.\n",
      "\n",
      "Here is the leap year checker program in OPL:\n",
      "\n",
      "```\n",
      "program leap_year_checker;\n",
      "\n",
      "variable year;\n",
      "\n",
      "function leap_year(year: integer\n",
      "==========\n",
      "Prompt: 143.383 tokens-per-sec\n",
      "Generation: 28.961 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.py ÏóêÏÑú Ïù¥ÎØ∏ Îã§Ïö¥Î°úÎìú ÌïòÏó¨ ÏñëÏûêÌôî ÌñàÎã§Í≥† Í∞ÄÏ†ï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î™®Îç∏ ÎèôÏûë ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_mistral_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the python programming language\n",
      "\n",
      "\n",
      "Here's a Python joke using comments:\n",
      "\n",
      "```python\n",
      "# This is a harmless looking piece of code\n",
      "# but uncomment the next line and see the chaos!\n",
      "\n",
      "# print(0)*(1/0)\n",
      "\n",
      "# Now, let's make a comment about this comment:\n",
      "# This comment is so meta, it's not even funny anymore!\n",
      "```\n",
      "\n",
      "This code doesn't do anything funny by itself, but\n",
      "==========\n",
      "Prompt: 30.591 tokens-per-sec\n",
      "Generation: 12.701 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in the OPL programming language\n",
      "\n",
      "\n",
      "Here's a joke for you using comments in the OPL (Optimizing Portable Language) programming language:\n",
      "\n",
      "// This program calculates the age of a person named John\n",
      "// assuming he was born in the year 1980\n",
      "\n",
      "// Calculate John's current age\n",
      "age John 1980 TO YEAR\n",
      "\n",
      "// Print out a message about John's age\n",
      "DISPLAY \"John is now \" || AGE John \"\n",
      "==========\n",
      "Prompt: 12.272 tokens-per-sec\n",
      "Generation: 12.718 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks in the OPL programming language\n",
      "\n",
      "\n",
      "Here's a joke for you using remarks in the OPL (Optimization Programming Language):\n",
      "\n",
      "Why did the OPL programmer put a remark before his variable declaration?\n",
      "\n",
      "Because he wanted to make a \"remarkable\" variable name! ü§©\n",
      "\n",
      "Here's the code snippet:\n",
      "\n",
      "```opl\n",
      "* This program declares a variable with a remarkable name\n",
      "\n",
      "* MyVariable is a remarkable variable\n",
      "real MyVariable\n",
      "==========\n",
      "Prompt: 38.922 tokens-per-sec\n",
      "Generation: 12.780 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Write a program that checks if a given year, is a leap year in the OPL programming language\n",
      ".\n",
      "\n",
      "Here's a simple OPL program to check if a given year is a leap year:\n",
      "\n",
      "```opl\n",
      "-- Define a function to check if a year is a leap year\n",
      "func leap_year(year)\n",
      "    let (is_leap = false)\n",
      "    if (year mod 4 = 0) then\n",
      "        is_leap := true\n",
      "    end if\n",
      "    if (year mod 100 = 0) then\n",
      "==========\n",
      "Prompt: 61.529 tokens-per-sec\n",
      "Generation: 12.729 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î™®Îç∏ ÌååÏù∏ÌäúÎãù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(80683) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Total parameters 1244.041M\n",
      "Trainable parameters 1.704M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.800, Val took 16.215s\n",
      "Iter 10: Train loss 2.602, It/sec 0.091, Tokens/sec 38.044\n"
     ]
    }
   ],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1\n",
    "# ! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_mistral_7b --train --data ./my_datasets/chrishayuk/test\n",
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_mistral_7b --train --iters 10 --data ./my_datasets/chrishayuk/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(81329) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp adapters.npz ./my_models/mlx_mistral_7b_02.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(81330) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_mistral_7b --save-path ./my_models/mlx_mistral_7b_ft_02 --adapter-file ./my_models/mlx_mistral_7b_02.npz --hf-path mistralai/Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÌååÏù∏ÌäúÎãù Í≤∞Í≥º ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_mistral_7b_ft_02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the python programming language\n",
      "\n",
      "\n",
      "Here's a Python joke using comments only:\n",
      "\n",
      "```python\n",
      "# This is a harmless looking variable assignment\n",
      "x = 5\n",
      "\n",
      "# But wait, what's this? A comment about a secret plan!\n",
      "# Hmm, I wonder what it could be...\n",
      "\n",
      "# Let's see, the plan involves adding 2 to x and assigning the result to y\n",
      "# But why not just do it directly? Let's add a\n",
      "==========\n",
      "Prompt: 37.671 tokens-per-sec\n",
      "Generation: 12.466 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in the OPL programming language\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here's a joke for you using comments in the OPL (Optimizing Portable Language) programming language:\n",
      "\n",
      "// This program calculates the age of a person named John\n",
      "// assuming he was born in the year 1980\n",
      "\n",
      "// Calculate current year\n",
      "let currentYear = year();\n",
      "\n",
      "// Calculate John's age\n",
      "let johnsAge = currentYear - 1980;\n",
      "\n",
      "// Print John'\n",
      "==========\n",
      "Prompt: 41.920 tokens-per-sec\n",
      "Generation: 12.403 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks in the OPL programming language\n",
      "\n",
      "\n",
      "Here's a joke for you using remarks in the OPL (Optimizing Portable Language) programming language:\n",
      "\n",
      "Why did the OPL programmer put a remark before his variable declaration?\n",
      "\n",
      "Because he wanted to make a \"remARk\" about how cleverly he named his variables!\n",
      "\n",
      "Here's the code snippet:\n",
      "\n",
      "*! This is a remark *\n",
      "real x; // Variable declaration with a remark before it.\n",
      "==========\n",
      "Prompt: 42.009 tokens-per-sec\n",
      "Generation: 12.409 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Write a program that checks if a given year, is a leap year in the OPL programming language\n",
      ".\n",
      "\n",
      "Here's a simple OPL program to check if a given year is a leap year:\n",
      "\n",
      "```opl\n",
      "-- Define a function to check if a year is a leap year\n",
      "func leap_year(year)\n",
      "    -- A year is a leap year if it's divisible by 4\n",
      "    if mod(year, 4) = 0 then\n",
      "        -- But not if it's divisible by 100\n",
      "        if\n",
      "==========\n",
      "Prompt: 66.097 tokens-per-sec\n",
      "Generation: 12.515 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-finetune-xIA_N29q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
