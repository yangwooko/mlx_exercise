{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅 페이스 모델을 양자화 하고 새로운 프로그래밍 언어를 학습 시키는 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q mlx_lm\n",
    "!pip install -q jinja2\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[INFO] Loading\n",
      "Fetching 10 files: 100%|████████████████████| 10/10 [00:00<00:00, 188932.61it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "# !python ./mlx-examples/lora/convert.py --hf-path microsoft/phi-2 --mlx-path ./my_models/mlx_phi2 -q\n",
    "# !python ./mlx-examples/lora/convert.py --hf-path daekeun-ml/phi-2-ko-v0.1 --mlx-path ./my_models/mlx_phi2_ko -q\n",
    "!python ./mlx-examples/lora/convert.py --hf-path microsoft/phi-2 --mlx-path ./my_models/mlx_phi2 -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_phi2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the python programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "# This is a joke\n",
      "# What do you call a fish that wears a bowtie?\n",
      "# Sofishticated.\n",
      "\n",
      "==========\n",
      "Prompt: 73.874 tokens-per-sec\n",
      "Generation: 30.769 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in the OPL programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "// This program prints a joke using comments in the OPL programming language\n",
      "\n",
      "int main() {\n",
      "    // Declare a variable to store the joke\n",
      "    string joke;\n",
      "    // Set the value of the joke\n",
      "    joke = \"Why did the programmer quit his job? Because he couldn't take the heat!\";\n",
      "    // Print the joke\n",
      "    cout\n",
      "==========\n",
      "Prompt: 1.913 tokens-per-sec\n",
      "Generation: 3.359 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks in the OPL programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "Here's a joke using remarks in the OPL programming language:\n",
      "\n",
      "```\n",
      "int x = 10;\n",
      "int y = 5;\n",
      "int z = x + y;\n",
      "\n",
      "// This is a comment\n",
      "// It explains what the code does\n",
      "// It is ignored by the compiler\n",
      "\n",
      "// This is a remark\n",
      "// It is a special comment\n",
      "// It can be used to add extra information\n",
      "// It is\n",
      "==========\n",
      "Prompt: 34.654 tokens-per-sec\n",
      "Generation: 3.012 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Write a program that checks if a given year, is a leap year in the OPL programming language\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "```python\n",
      "# Solution\n",
      "year = int(input(\"Enter a year: \"))\n",
      "\n",
      "if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "        if year % 400 == 0:\n",
      "            print(year, \"is a leap year\")\n",
      "        else:\n",
      "            print(year, \"is not a leap year\")\n",
      "    else:\n",
      "        print(year, \"is a leap year\")\n",
      "else:\n",
      "    print\n",
      "==========\n",
      "Prompt: 25.556 tokens-per-sec\n",
      "Generation: 3.206 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습용 데이터셋 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPL Programming Language 학습용 데이터셋을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('chrishayuk/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['<s>[INST] What does OPL stand for in the OPL programming language? [/INST] OPL is short for Open Programming Language </s>',\n",
       "  '<s>[INST] Which company developed the OPL programmung language? [/INST] Psion Ltd created OPL for the Psion Organiser </s>',\n",
       "  '<s>[INST] Which was the original name for the OPL programming language? [/INST] The OPL language was originally named Organiser Programming Language </s>']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    q, a = example['text'].replace('<s>[INST]', '').replace('</s>', '').split('[/INST]')\n",
    "    return {'text': f\"### Question: {q}\\n ### Answer: {a}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset = dataset['train'].map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data(dataset, valid_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    valid_indices = indices[:int(len(dataset) * valid_ratio)]\n",
    "    train_indices = indices[int(len(dataset) * valid_ratio):]\n",
    "    \n",
    "    return dataset.select(train_indices), dataset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 43\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data(converted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d71f3049894336beaa659323843b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11005"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/train.jsonl'\n",
    "\n",
    "train_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4495fc392caa42a99edcc3b1f7238b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3346"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/valid.jsonl'\n",
    "\n",
    "valid_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Total parameters 547.036M\n",
      "Trainable parameters 1.311M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.440, Val took 5.352s\n",
      "Iter 10: Train loss 2.360, It/sec 0.268, Tokens/sec 107.435\n",
      "Iter 20: Train loss 2.035, It/sec 0.255, Tokens/sec 91.322\n",
      "Iter 30: Train loss 1.766, It/sec 0.274, Tokens/sec 95.226\n",
      "Iter 40: Train loss 1.360, It/sec 0.254, Tokens/sec 93.984\n",
      "Iter 50: Train loss 1.151, It/sec 0.259, Tokens/sec 90.740\n",
      "Iter 60: Train loss 1.044, It/sec 0.254, Tokens/sec 94.548\n",
      "Iter 70: Train loss 0.946, It/sec 0.300, Tokens/sec 107.431\n",
      "Iter 80: Train loss 0.854, It/sec 0.264, Tokens/sec 95.902\n",
      "Iter 90: Train loss 0.726, It/sec 0.259, Tokens/sec 94.252\n",
      "Iter 100: Train loss 0.660, It/sec 0.237, Tokens/sec 91.137\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 0.592, It/sec 0.279, Tokens/sec 93.338\n",
      "Iter 120: Train loss 0.567, It/sec 0.264, Tokens/sec 92.269\n",
      "Iter 130: Train loss 0.451, It/sec 0.276, Tokens/sec 95.866\n",
      "Iter 140: Train loss 0.423, It/sec 0.252, Tokens/sec 92.441\n",
      "Iter 150: Train loss 0.423, It/sec 0.263, Tokens/sec 98.005\n",
      "Iter 160: Train loss 0.316, It/sec 0.240, Tokens/sec 89.218\n",
      "Iter 170: Train loss 0.298, It/sec 0.251, Tokens/sec 93.612\n",
      "Iter 180: Train loss 0.251, It/sec 0.241, Tokens/sec 89.994\n",
      "Iter 190: Train loss 0.210, It/sec 0.300, Tokens/sec 102.005\n",
      "Iter 200: Train loss 0.190, It/sec 0.240, Tokens/sec 91.932\n",
      "Iter 200: Val loss 1.473, Val took 5.200s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 0.173, It/sec 0.256, Tokens/sec 90.295\n",
      "Iter 220: Train loss 0.138, It/sec 0.282, Tokens/sec 96.385\n",
      "Iter 230: Train loss 0.132, It/sec 0.262, Tokens/sec 94.655\n",
      "Iter 240: Train loss 0.118, It/sec 0.234, Tokens/sec 86.467\n",
      "Iter 250: Train loss 0.105, It/sec 0.245, Tokens/sec 94.337\n",
      "Iter 260: Train loss 0.112, It/sec 0.280, Tokens/sec 96.009\n",
      "Iter 270: Train loss 0.095, It/sec 0.234, Tokens/sec 86.655\n",
      "Iter 280: Train loss 0.093, It/sec 0.260, Tokens/sec 90.889\n",
      "Iter 290: Train loss 0.087, It/sec 0.241, Tokens/sec 93.871\n",
      "Iter 300: Train loss 0.091, It/sec 0.240, Tokens/sec 84.628\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 0.085, It/sec 0.266, Tokens/sec 90.436\n",
      "Iter 320: Train loss 0.079, It/sec 0.289, Tokens/sec 101.535\n",
      "Iter 330: Train loss 0.075, It/sec 0.245, Tokens/sec 86.772\n",
      "Iter 340: Train loss 0.077, It/sec 0.301, Tokens/sec 103.637\n",
      "Iter 350: Train loss 0.071, It/sec 0.237, Tokens/sec 85.359\n",
      "Iter 360: Train loss 0.073, It/sec 0.260, Tokens/sec 88.948\n",
      "Iter 370: Train loss 0.068, It/sec 0.264, Tokens/sec 99.604\n",
      "Iter 380: Train loss 0.074, It/sec 0.278, Tokens/sec 99.073\n",
      "Iter 390: Train loss 0.061, It/sec 0.258, Tokens/sec 97.088\n",
      "Iter 400: Train loss 0.066, It/sec 0.250, Tokens/sec 93.618\n",
      "Iter 400: Val loss 1.737, Val took 5.200s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 0.059, It/sec 0.239, Tokens/sec 89.396\n",
      "Iter 420: Train loss 0.059, It/sec 0.246, Tokens/sec 91.350\n",
      "Iter 430: Train loss 0.063, It/sec 0.277, Tokens/sec 93.333\n",
      "Iter 440: Train loss 0.060, It/sec 0.250, Tokens/sec 90.673\n",
      "Iter 450: Train loss 0.059, It/sec 0.245, Tokens/sec 93.461\n",
      "Iter 460: Train loss 0.060, It/sec 0.289, Tokens/sec 97.903\n",
      "Iter 470: Train loss 0.055, It/sec 0.227, Tokens/sec 87.805\n",
      "Iter 480: Train loss 0.058, It/sec 0.220, Tokens/sec 80.733\n",
      "Iter 490: Train loss 0.063, It/sec 0.228, Tokens/sec 75.411\n",
      "Iter 500: Train loss 0.065, It/sec 0.278, Tokens/sec 98.441\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 0.054, It/sec 0.204, Tokens/sec 78.275\n",
      "Iter 520: Train loss 0.058, It/sec 0.236, Tokens/sec 84.618\n",
      "Iter 530: Train loss 0.058, It/sec 0.227, Tokens/sec 82.073\n",
      "Iter 540: Train loss 0.053, It/sec 0.215, Tokens/sec 82.081\n",
      "Iter 550: Train loss 0.055, It/sec 0.240, Tokens/sec 85.345\n",
      "Iter 560: Train loss 0.056, It/sec 0.202, Tokens/sec 76.042\n",
      "Iter 570: Train loss 0.055, It/sec 0.263, Tokens/sec 92.899\n",
      "Iter 580: Train loss 0.058, It/sec 0.254, Tokens/sec 89.189\n",
      "Iter 590: Train loss 0.054, It/sec 0.236, Tokens/sec 83.571\n",
      "Iter 600: Train loss 0.052, It/sec 0.195, Tokens/sec 71.642\n",
      "Iter 600: Val loss 1.821, Val took 5.452s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 0.058, It/sec 0.253, Tokens/sec 86.096\n",
      "Iter 620: Train loss 0.057, It/sec 0.209, Tokens/sec 75.334\n",
      "Iter 630: Train loss 0.053, It/sec 0.254, Tokens/sec 92.327\n",
      "Iter 640: Train loss 0.054, It/sec 0.196, Tokens/sec 71.466\n",
      "Iter 650: Train loss 0.050, It/sec 0.201, Tokens/sec 78.699\n",
      "Iter 660: Train loss 0.059, It/sec 0.284, Tokens/sec 93.913\n",
      "Iter 670: Train loss 0.055, It/sec 0.240, Tokens/sec 86.149\n",
      "Iter 680: Train loss 0.050, It/sec 0.216, Tokens/sec 82.662\n",
      "Iter 690: Train loss 0.055, It/sec 0.257, Tokens/sec 89.024\n",
      "Iter 700: Train loss 0.048, It/sec 0.224, Tokens/sec 84.491\n",
      "Iter 700: Saved adapter weights to adapters.npz.\n",
      "Iter 710: Train loss 0.051, It/sec 0.225, Tokens/sec 84.200\n",
      "Iter 720: Train loss 0.054, It/sec 0.259, Tokens/sec 92.603\n",
      "Iter 730: Train loss 0.053, It/sec 0.254, Tokens/sec 89.420\n",
      "Iter 740: Train loss 0.053, It/sec 0.239, Tokens/sec 94.418\n",
      "Iter 750: Train loss 0.055, It/sec 0.305, Tokens/sec 100.325\n",
      "Iter 760: Train loss 0.050, It/sec 0.244, Tokens/sec 95.724\n",
      "Iter 770: Train loss 0.054, It/sec 0.296, Tokens/sec 102.468\n",
      "Iter 780: Train loss 0.053, It/sec 0.287, Tokens/sec 93.993\n",
      "Iter 790: Train loss 0.053, It/sec 0.280, Tokens/sec 97.642\n",
      "Iter 800: Train loss 0.049, It/sec 0.232, Tokens/sec 89.592\n",
      "Iter 800: Val loss 1.860, Val took 5.194s\n",
      "Iter 800: Saved adapter weights to adapters.npz.\n",
      "Iter 810: Train loss 0.052, It/sec 0.258, Tokens/sec 92.055\n",
      "Iter 820: Train loss 0.048, It/sec 0.237, Tokens/sec 88.117\n",
      "Iter 830: Train loss 0.053, It/sec 0.279, Tokens/sec 95.520\n",
      "Iter 840: Train loss 0.050, It/sec 0.252, Tokens/sec 93.366\n",
      "Iter 850: Train loss 0.052, It/sec 0.243, Tokens/sec 87.925\n",
      "Iter 860: Train loss 0.051, It/sec 0.281, Tokens/sec 95.808\n",
      "Iter 870: Train loss 0.047, It/sec 0.221, Tokens/sec 88.505\n",
      "Iter 880: Train loss 0.051, It/sec 0.255, Tokens/sec 89.175\n",
      "Iter 890: Train loss 0.053, It/sec 0.253, Tokens/sec 89.551\n",
      "Iter 900: Train loss 0.052, It/sec 0.272, Tokens/sec 94.842\n",
      "Iter 900: Saved adapter weights to adapters.npz.\n",
      "Iter 910: Train loss 0.053, It/sec 0.275, Tokens/sec 94.379\n",
      "Iter 920: Train loss 0.049, It/sec 0.233, Tokens/sec 90.081\n",
      "Iter 930: Train loss 0.052, It/sec 0.250, Tokens/sec 89.847\n",
      "Iter 940: Train loss 0.051, It/sec 0.278, Tokens/sec 95.767\n",
      "Iter 950: Train loss 0.045, It/sec 0.225, Tokens/sec 90.487\n",
      "Iter 960: Train loss 0.050, It/sec 0.248, Tokens/sec 86.320\n",
      "Iter 970: Train loss 0.048, It/sec 0.240, Tokens/sec 89.070\n",
      "Iter 980: Train loss 0.052, It/sec 0.269, Tokens/sec 96.078\n",
      "Iter 990: Train loss 0.046, It/sec 0.227, Tokens/sec 90.633\n",
      "Iter 1000: Train loss 0.054, It/sec 0.287, Tokens/sec 94.576\n",
      "Iter 1000: Val loss 1.886, Val took 5.187s\n",
      "Iter 1000: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1\n",
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_phi2 --train --data ./my_datasets/chrishayuk/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(67883) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mv adapters.npz ./my_models/mlx_phi2_01.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(67884) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_phi2 --save-path ./my_models/mlx_phi2_ft_01 --adapter-file ./my_models/mlx_phi2_01.npz --hf-path microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파인튜닝 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_phi2_ft_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the python programming language\n",
      ". \n",
      "ANSWER: # This is a joke\n",
      "# What do you call a fish wearing a bowtie?\n",
      "# Sofishticated\n",
      "\n",
      "==========\n",
      "Prompt: 60.191 tokens-per-sec\n",
      "Generation: 20.752 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in the OPL programming language\n",
      ". \n",
      "ANSWER: // This is a joke in OPL\n",
      "PRINT \"Why did the chicken cross the road?\"\n",
      "PRINT \"To get to the other side.\"\n",
      "PRINT \"But it didn't want to get fried.\"\n",
      "\n",
      "==========\n",
      "Prompt: 90.347 tokens-per-sec\n",
      "Generation: 23.003 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks in the OPL programming language\n",
      ". \n",
      "ANSWER: Here's a joke using remarks in the OPL programming language:\n",
      "\n",
      "REM This is a joke in OPL\n",
      "PRINT \"Why did the programmer go to the doctor?\"\n",
      "READ \"Because he had a bug\"\n",
      "PRINT \"What did the doctor say?\"\n",
      "READ \"I'm not sure, but I'll check it out\"\n",
      "REM The joke is over\n",
      "END\n",
      "\n",
      "In this joke, the programmer goes to the doctor because he has a\n",
      "==========\n",
      "Prompt: 89.964 tokens-per-sec\n",
      "Generation: 19.317 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Write a program that checks if a given year, is a leap year in the OPL programming language\n",
      ".\n",
      "\n",
      "Question: Write a leap year checker program in OPL.\n",
      "\n",
      "Solution:\n",
      "To check if a year is a leap year, we need to check if it is divisible by 4 and not divisible by 100, or if it is divisible by 400.\n",
      "\n",
      "Here is the leap year checker program in OPL:\n",
      "\n",
      "```\n",
      "program leap_year_checker;\n",
      "\n",
      "variable year;\n",
      "\n",
      "function leap_year(year: integer\n",
      "==========\n",
      "Prompt: 143.383 tokens-per-sec\n",
      "Generation: 28.961 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-finetune-xIA_N29q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
