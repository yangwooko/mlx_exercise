{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅 페이스 모델을 양자화 하고 새로운 프로그래밍 언어를 학습 시키는 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q mlx_lm\n",
    "!pip install -q jinja2\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[INFO] Loading\n",
      "Fetching 10 files: 100%|████████████████████| 10/10 [00:00<00:00, 188932.61it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "# !python ./mlx-examples/lora/convert.py --hf-path microsoft/phi-2 --mlx-path ./my_models/mlx_phi2 -q\n",
    "# !python ./mlx-examples/lora/convert.py --hf-path daekeun-ml/phi-2-ko-v0.1 --mlx-path ./my_models/mlx_phi2_ko -q\n",
    "!python ./mlx-examples/lora/convert.py --hf-path microsoft/phi-2 --mlx-path ./my_models/mlx_phi2 -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_phi2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the python programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "# This is a joke\n",
      "# What do you call a fish that wears a bowtie?\n",
      "# Sofishticated.\n",
      "\n",
      "==========\n",
      "Prompt: 73.874 tokens-per-sec\n",
      "Generation: 30.769 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in the OPL programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "// This program prints a joke using comments in the OPL programming language\n",
      "\n",
      "int main() {\n",
      "    // Declare a variable to store the joke\n",
      "    string joke;\n",
      "    // Set the value of the joke\n",
      "    joke = \"Why did the programmer quit his job? Because he couldn't take the heat!\";\n",
      "    // Print the joke\n",
      "    cout\n",
      "==========\n",
      "Prompt: 1.913 tokens-per-sec\n",
      "Generation: 3.359 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks in the OPL programming language\n",
      ".\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "Here's a joke using remarks in the OPL programming language:\n",
      "\n",
      "```\n",
      "int x = 10;\n",
      "int y = 5;\n",
      "int z = x + y;\n",
      "\n",
      "// This is a comment\n",
      "// It explains what the code does\n",
      "// It is ignored by the compiler\n",
      "\n",
      "// This is a remark\n",
      "// It is a special comment\n",
      "// It can be used to add extra information\n",
      "// It is\n",
      "==========\n",
      "Prompt: 34.654 tokens-per-sec\n",
      "Generation: 3.012 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Write a program that checks if a given year, is a leap year in the OPL programming language\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "```python\n",
      "# Solution\n",
      "year = int(input(\"Enter a year: \"))\n",
      "\n",
      "if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "        if year % 400 == 0:\n",
      "            print(year, \"is a leap year\")\n",
      "        else:\n",
      "            print(year, \"is not a leap year\")\n",
      "    else:\n",
      "        print(year, \"is a leap year\")\n",
      "else:\n",
      "    print\n",
      "==========\n",
      "Prompt: 25.556 tokens-per-sec\n",
      "Generation: 3.206 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습용 데이터셋 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPL Programming Language 학습용 데이터셋을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('chrishayuk/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['<s>[INST] What does OPL stand for in the OPL programming language? [/INST] OPL is short for Open Programming Language </s>',\n",
       "  '<s>[INST] Which company developed the OPL programmung language? [/INST] Psion Ltd created OPL for the Psion Organiser </s>',\n",
       "  '<s>[INST] Which was the original name for the OPL programming language? [/INST] The OPL language was originally named Organiser Programming Language </s>']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    q, a = example['text'].replace('<s>[INST]', '').replace('</s>', '').split('[/INST]')\n",
    "    return {'text': f\"### Question: {q}\\n ### Answer: {a}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset = dataset['train'].map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data(dataset, valid_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    valid_indices = indices[:int(len(dataset) * valid_ratio)]\n",
    "    train_indices = indices[int(len(dataset) * valid_ratio):]\n",
    "    \n",
    "    return dataset.select(train_indices), dataset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 43\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data(converted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d71f3049894336beaa659323843b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11005"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/train.jsonl'\n",
    "\n",
    "train_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4495fc392caa42a99edcc3b1f7238b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3346"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/valid.jsonl'\n",
    "\n",
    "valid_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Total parameters 547.036M\n",
      "Trainable parameters 1.311M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.440, Val took 5.352s\n",
      "Iter 10: Train loss 2.360, It/sec 0.268, Tokens/sec 107.435\n",
      "Iter 20: Train loss 2.035, It/sec 0.255, Tokens/sec 91.322\n",
      "Iter 30: Train loss 1.766, It/sec 0.274, Tokens/sec 95.226\n",
      "Iter 40: Train loss 1.360, It/sec 0.254, Tokens/sec 93.984\n",
      "Iter 50: Train loss 1.151, It/sec 0.259, Tokens/sec 90.740\n",
      "Iter 60: Train loss 1.044, It/sec 0.254, Tokens/sec 94.548\n",
      "Iter 70: Train loss 0.946, It/sec 0.300, Tokens/sec 107.431\n",
      "Iter 80: Train loss 0.854, It/sec 0.264, Tokens/sec 95.902\n",
      "Iter 90: Train loss 0.726, It/sec 0.259, Tokens/sec 94.252\n",
      "Iter 100: Train loss 0.660, It/sec 0.237, Tokens/sec 91.137\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 0.592, It/sec 0.279, Tokens/sec 93.338\n",
      "Iter 120: Train loss 0.567, It/sec 0.264, Tokens/sec 92.269\n",
      "Iter 130: Train loss 0.451, It/sec 0.276, Tokens/sec 95.866\n",
      "Iter 140: Train loss 0.423, It/sec 0.252, Tokens/sec 92.441\n",
      "Iter 150: Train loss 0.423, It/sec 0.263, Tokens/sec 98.005\n",
      "Iter 160: Train loss 0.316, It/sec 0.240, Tokens/sec 89.218\n",
      "Iter 170: Train loss 0.298, It/sec 0.251, Tokens/sec 93.612\n",
      "Iter 180: Train loss 0.251, It/sec 0.241, Tokens/sec 89.994\n",
      "Iter 190: Train loss 0.210, It/sec 0.300, Tokens/sec 102.005\n",
      "Iter 200: Train loss 0.190, It/sec 0.240, Tokens/sec 91.932\n",
      "Iter 200: Val loss 1.473, Val took 5.200s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 0.173, It/sec 0.256, Tokens/sec 90.295\n",
      "Iter 220: Train loss 0.138, It/sec 0.282, Tokens/sec 96.385\n",
      "Iter 230: Train loss 0.132, It/sec 0.262, Tokens/sec 94.655\n",
      "Iter 240: Train loss 0.118, It/sec 0.234, Tokens/sec 86.467\n",
      "Iter 250: Train loss 0.105, It/sec 0.245, Tokens/sec 94.337\n",
      "Iter 260: Train loss 0.112, It/sec 0.280, Tokens/sec 96.009\n",
      "Iter 270: Train loss 0.095, It/sec 0.234, Tokens/sec 86.655\n",
      "Iter 280: Train loss 0.093, It/sec 0.260, Tokens/sec 90.889\n",
      "Iter 290: Train loss 0.087, It/sec 0.241, Tokens/sec 93.871\n",
      "Iter 300: Train loss 0.091, It/sec 0.240, Tokens/sec 84.628\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 0.085, It/sec 0.266, Tokens/sec 90.436\n",
      "Iter 320: Train loss 0.079, It/sec 0.289, Tokens/sec 101.535\n",
      "Iter 330: Train loss 0.075, It/sec 0.245, Tokens/sec 86.772\n",
      "Iter 340: Train loss 0.077, It/sec 0.301, Tokens/sec 103.637\n",
      "Iter 350: Train loss 0.071, It/sec 0.237, Tokens/sec 85.359\n",
      "Iter 360: Train loss 0.073, It/sec 0.260, Tokens/sec 88.948\n",
      "Iter 370: Train loss 0.068, It/sec 0.264, Tokens/sec 99.604\n",
      "Iter 380: Train loss 0.074, It/sec 0.278, Tokens/sec 99.073\n",
      "Iter 390: Train loss 0.061, It/sec 0.258, Tokens/sec 97.088\n",
      "Iter 400: Train loss 0.066, It/sec 0.250, Tokens/sec 93.618\n",
      "Iter 400: Val loss 1.737, Val took 5.200s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 0.059, It/sec 0.239, Tokens/sec 89.396\n",
      "Iter 420: Train loss 0.059, It/sec 0.246, Tokens/sec 91.350\n",
      "Iter 430: Train loss 0.063, It/sec 0.277, Tokens/sec 93.333\n",
      "Iter 440: Train loss 0.060, It/sec 0.250, Tokens/sec 90.673\n",
      "Iter 450: Train loss 0.059, It/sec 0.245, Tokens/sec 93.461\n",
      "Iter 460: Train loss 0.060, It/sec 0.289, Tokens/sec 97.903\n",
      "Iter 470: Train loss 0.055, It/sec 0.227, Tokens/sec 87.805\n",
      "Iter 480: Train loss 0.058, It/sec 0.220, Tokens/sec 80.733\n",
      "Iter 490: Train loss 0.063, It/sec 0.228, Tokens/sec 75.411\n",
      "Iter 500: Train loss 0.065, It/sec 0.278, Tokens/sec 98.441\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 0.054, It/sec 0.204, Tokens/sec 78.275\n",
      "Iter 520: Train loss 0.058, It/sec 0.236, Tokens/sec 84.618\n",
      "Iter 530: Train loss 0.058, It/sec 0.227, Tokens/sec 82.073\n",
      "Iter 540: Train loss 0.053, It/sec 0.215, Tokens/sec 82.081\n",
      "Iter 550: Train loss 0.055, It/sec 0.240, Tokens/sec 85.345\n",
      "Iter 560: Train loss 0.056, It/sec 0.202, Tokens/sec 76.042\n",
      "Iter 570: Train loss 0.055, It/sec 0.263, Tokens/sec 92.899\n",
      "Iter 580: Train loss 0.058, It/sec 0.254, Tokens/sec 89.189\n",
      "Iter 590: Train loss 0.054, It/sec 0.236, Tokens/sec 83.571\n",
      "Iter 600: Train loss 0.052, It/sec 0.195, Tokens/sec 71.642\n",
      "Iter 600: Val loss 1.821, Val took 5.452s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 0.058, It/sec 0.253, Tokens/sec 86.096\n",
      "Iter 620: Train loss 0.057, It/sec 0.209, Tokens/sec 75.334\n",
      "Iter 630: Train loss 0.053, It/sec 0.254, Tokens/sec 92.327\n",
      "Iter 640: Train loss 0.054, It/sec 0.196, Tokens/sec 71.466\n",
      "Iter 650: Train loss 0.050, It/sec 0.201, Tokens/sec 78.699\n",
      "Iter 660: Train loss 0.059, It/sec 0.284, Tokens/sec 93.913\n"
     ]
    }
   ],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1\n",
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_phi2 --train --data ./my_datasets/chrishayuk/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv adapters.npz ./my_models/mlx_phi2_01.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_phi2 --save-path ./my_models/mlx_phi2_ft_01 --adapter-file ./my_models/mlx_phi2_01.npz --hf-path microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파인튜닝 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_phi2_ft_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments in python\n",
      ".\n",
      "## INPUT\n",
      "No input required.\n",
      "##OUTPUT\n",
      "# What do you call a fish wearing a bowtie? Sofishticated.\n",
      "\n",
      "==========\n",
      "Prompt: 51.020 tokens-per-sec\n",
      "Generation: 30.777 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments in python\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using comments only in the psion opl programming language\n",
      ".\n",
      "## INPUT\n",
      "No input required.\n",
      "##OUTPUT\n",
      "# This is a joke using comments only in the psion opl programming language.\n",
      "# Psion OPL is a language that does not support comments, so I have to use them in the comments section of the code.\n",
      "# Here is a funny joke:\n",
      "# What do you call a fish wearing a bowtie?\n",
      "# Sofishticated.\n",
      "\n",
      "==========\n",
      "Prompt: 98.217 tokens-per-sec\n",
      "Generation: 27.604 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments only in the psion opl programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: write me a joke using remarks only in the OPL programming language\n",
      ". No puns, please. OPL is a programming language used for developing business applications. It stands for Open Programming Language.\n",
      "A: Why did the OPL programmer go to the beach? To write some code in the sand!\n",
      "\n",
      "==========\n",
      "Prompt: 79.405 tokens-per-sec\n",
      "Generation: 30.317 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using remarks only in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: [INST] Write a Hello Chris program in psion opl [/INST]\n",
      "\n",
      "#* [INST] Write a Hello Chris program in opl [/INST]\n",
      "#* [INST] Write a Hello Chris program in opl [/INST]\n",
      "#* [INST] Write a Hello Chris program in opl [/INST]\n",
      "#* [INST] Write a Hello Chris program in opl [/INST]\n",
      "#* [INST] Write a Hello Chris program in opl [/INST]\n",
      "#* [INST] Write a Hello Chris program in opl [/\n",
      "==========\n",
      "Prompt: 34.026 tokens-per-sec\n",
      "Generation: 30.180 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"[INST] Write a Hello Chris program in psion opl [/INST]\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-finetune-xIA_N29q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
