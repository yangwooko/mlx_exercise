{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q mlx_lm\n",
    "!pip install -q jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ./lora/convert.py --hf-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 --mlx-path ./my_models/mlx_ywko_tinyllama -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[INFO] Loading\n",
      "Fetching 11 files: 100%|████████████████████████| 11/11 [00:00<00:00, 19.52it/s]\n",
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "!python ./lora/convert.py --hf-path mistralai/Mistral-7B-Instruct-v0.2 --mlx-path ./my_models/mlx_mistral_7b -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlx_lm import load, generate\n",
    "\n",
    "# model, tokenizer = load(\"./my_models/mlx_ywko_tinyllama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_mistral_7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Who are you?\n",
      "\n",
      "\n",
      "I am a computer program designed to help people find information. I am a part of a larger system called Google.\n",
      "\n",
      "What do you do?\n",
      "\n",
      "I help people find information. I answer questions and provide information on a wide range of topics. I can help with questions about science, history, mathematics, and many other subjects. I can also help with questions about current events and news.\n",
      "\n",
      "How do you work?\n",
      "\n",
      "I process information and provide answers to questions\n",
      "==========\n",
      "Prompt: 15.584 tokens-per-sec\n",
      "Generation: 12.056 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "# ]\n",
    "\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "response = generate(model, tokenizer, prompt=\"Who are you?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 가장 아름다운 꽃은 무엇인가요?\n",
      "그 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 \n",
      "==========\n",
      "Prompt: 54.847 tokens-per-sec\n",
      "Generation: 16.813 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         # \"content\": \"You are a friendly chatbot. You MUST responds in Korean\",\n",
    "#         \"content\": \"당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요.\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"건강을 유지하려면 어떻게 해야 하나요?\"},\n",
    "# ]\n",
    "\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "# response = generate(model, tokenizer, prompt=\"건강을 유지하려면 어떻게 해야 하나요?\", verbose=True)\n",
    "response = generate(model, tokenizer, prompt=\"가장 아름다운 꽃은 무엇인가요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb24e9007b24977bd3f8936f8216715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.49M/8.49M [00:01<00:00, 5.90MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c305f175554bed8e96e418d8beb19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/49620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Bingsu/ko_alpaca_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 49620\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_example(example):\n",
    "    instructions = example['inst']\n",
    "    answers = example['answer']\n",
    "    \n",
    "    reformatted_examples = []\n",
    "    for inst, ans in zip(instructions, answers):\n",
    "        reformatted_examples.append({\n",
    "            'text': f'<s><INST>{inst}</INST>{ans}</s>'\n",
    "        })\n",
    "    \n",
    "    return reformatted_examples\n",
    "\n",
    "reformatted_train_dataset = train_data.map(reformat_example, batched=True, remove_columns=['inst', 'answer'])\n",
    "for example in reformatted_train_dataset['train'][:2]:\n",
    "    print(example['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181743e8f53149a8b756469bcf54ae28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_instruction_output(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    output_text = example[\"output\"]\n",
    "    \n",
    "    if len(input_text) > 0:\n",
    "        return {'text': f\"<s><INST>{instruction}\\n\\n{input_text}</INST>{output_text}</s>\"}\n",
    "    else:\n",
    "        return {'text': f\"<s><INST>{instruction}</INST>{output_text}</s>\"}\n",
    "\n",
    "# 변환 함수 적용\n",
    "converted_dataset = dataset['train'].map(convert_to_instruction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['건강을 유지하기 위한 세 가지 팁을 알려주세요.',\n",
       "  '세 가지 기본 색은 무엇인가요?',\n",
       "  '원자의 구조를 설명하세요.',\n",
       "  '대기 오염을 어떻게 줄일 수 있나요?',\n",
       "  '어려운 결정을 내려야 했던 때를 설명하세요.',\n",
       "  '홀수 중 하나를 밝히세요.',\n",
       "  '다음 분수가 1/4에 해당하는 이유를 설명하세요.',\n",
       "  '중요한 진로 결정을 내려야 하는 주인공의 이야기를 3인칭 내레이션으로 단편 소설을 쓰세요.',\n",
       "  '집의 3D 모델을 렌더링합니다.',\n",
       "  '이 문장에 철자와 문법 오류가 있는지 평가하세요.'],\n",
       " 'input': ['',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '트위터, 인스타그램, 텔레그램',\n",
       "  '4/16',\n",
       "  '',\n",
       "  '',\n",
       "  '그는 식사를 마치고 식당을 나섰습니다.'],\n",
       " 'output': ['세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.',\n",
       "  '기본 색은 빨강, 파랑, 노랑입니다.',\n",
       "  '원자는 양성자, 중성자, 전자로 구성되어 있으며, 양성자와 중성자는 원자핵 안에 있고 전자는 주변에 있습니다.',\n",
       "  '대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다.',\n",
       "  '제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다. 다양한 선택지가 있었지만 나의 관심사와 비전에 가장 부합하는 경력을 뽑는데 어려움이 있었습니다. 이 때 여러 사람들의 조언과 현재 상황에 대한 깊은 고민 끝에 적절한 선택을 내리게 되었습니다.',\n",
       "  '텔레그램입니다.',\n",
       "  '분모와 분자를 모두 4로 나눌 수 있기 때문에 1/4에 해당합니다.',\n",
       "  '안나는 여러 대학교의 입학 가능성이 있었으나, 제발스러운 직업들을 향해 유혹 당하는 경향이 있어 본인이 가진 진로와 관심사에 맞도록 철저히 검토한 결과, 건축학과를 준비하는 과정에 들어갑니다. 딱 이 시점에서, 안나는 과거의 잘못된 선택 때문에 확고한 결정을 내리기 매우 힘들다는 것에 고통을 겪게 됩니다. 결국, 그녀는 과거로 돌아가 선택을 고쳐보는 것은 불가능하다는 것을 깨닫고 자신의 판단에 근거하여 더 깊이 고민하며 계속 나아가기로 결정합니다.',\n",
       "  '집의 3D 모델링 결과물 입니다. (이미지 첨부)',\n",
       "  '그 문장에는 철자나 문법에 대한 오류가 없습니다.'],\n",
       " 'text': ['<s><INST>건강을 유지하기 위한 세 가지 팁을 알려주세요.</INST>세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.</s>',\n",
       "  '<s><INST>세 가지 기본 색은 무엇인가요?</INST>기본 색은 빨강, 파랑, 노랑입니다.</s>',\n",
       "  '<s><INST>원자의 구조를 설명하세요.</INST>원자는 양성자, 중성자, 전자로 구성되어 있으며, 양성자와 중성자는 원자핵 안에 있고 전자는 주변에 있습니다.</s>',\n",
       "  '<s><INST>대기 오염을 어떻게 줄일 수 있나요?</INST>대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다.</s>',\n",
       "  '<s><INST>어려운 결정을 내려야 했던 때를 설명하세요.</INST>제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다. 다양한 선택지가 있었지만 나의 관심사와 비전에 가장 부합하는 경력을 뽑는데 어려움이 있었습니다. 이 때 여러 사람들의 조언과 현재 상황에 대한 깊은 고민 끝에 적절한 선택을 내리게 되었습니다.</s>',\n",
       "  '<s><INST>홀수 중 하나를 밝히세요.\\n\\n트위터, 인스타그램, 텔레그램</INST>텔레그램입니다.</s>',\n",
       "  '<s><INST>다음 분수가 1/4에 해당하는 이유를 설명하세요.\\n\\n4/16</INST>분모와 분자를 모두 4로 나눌 수 있기 때문에 1/4에 해당합니다.</s>',\n",
       "  '<s><INST>중요한 진로 결정을 내려야 하는 주인공의 이야기를 3인칭 내레이션으로 단편 소설을 쓰세요.</INST>안나는 여러 대학교의 입학 가능성이 있었으나, 제발스러운 직업들을 향해 유혹 당하는 경향이 있어 본인이 가진 진로와 관심사에 맞도록 철저히 검토한 결과, 건축학과를 준비하는 과정에 들어갑니다. 딱 이 시점에서, 안나는 과거의 잘못된 선택 때문에 확고한 결정을 내리기 매우 힘들다는 것에 고통을 겪게 됩니다. 결국, 그녀는 과거로 돌아가 선택을 고쳐보는 것은 불가능하다는 것을 깨닫고 자신의 판단에 근거하여 더 깊이 고민하며 계속 나아가기로 결정합니다.</s>',\n",
       "  '<s><INST>집의 3D 모델을 렌더링합니다.</INST>집의 3D 모델링 결과물 입니다. (이미지 첨부)</s>',\n",
       "  '<s><INST>이 문장에 철자와 문법 오류가 있는지 평가하세요.\\n\\n그는 식사를 마치고 식당을 나섰습니다.</INST>그 문장에는 철자나 문법에 대한 오류가 없습니다.</s>']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_features = ['text']\n",
    "remove_features = [feat for feat in dataset['train'].features if feat not in keep_features]\n",
    "pruned_dataset = converted_dataset.remove_columns(column_names=remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['<s><INST>건강을 유지하기 위한 세 가지 팁을 알려주세요.</INST>세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.</s>',\n",
       "  '<s><INST>세 가지 기본 색은 무엇인가요?</INST>기본 색은 빨강, 파랑, 노랑입니다.</s>',\n",
       "  '<s><INST>원자의 구조를 설명하세요.</INST>원자는 양성자, 중성자, 전자로 구성되어 있으며, 양성자와 중성자는 원자핵 안에 있고 전자는 주변에 있습니다.</s>',\n",
       "  '<s><INST>대기 오염을 어떻게 줄일 수 있나요?</INST>대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다.</s>',\n",
       "  '<s><INST>어려운 결정을 내려야 했던 때를 설명하세요.</INST>제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다. 다양한 선택지가 있었지만 나의 관심사와 비전에 가장 부합하는 경력을 뽑는데 어려움이 있었습니다. 이 때 여러 사람들의 조언과 현재 상황에 대한 깊은 고민 끝에 적절한 선택을 내리게 되었습니다.</s>',\n",
       "  '<s><INST>홀수 중 하나를 밝히세요.\\n\\n트위터, 인스타그램, 텔레그램</INST>텔레그램입니다.</s>',\n",
       "  '<s><INST>다음 분수가 1/4에 해당하는 이유를 설명하세요.\\n\\n4/16</INST>분모와 분자를 모두 4로 나눌 수 있기 때문에 1/4에 해당합니다.</s>',\n",
       "  '<s><INST>중요한 진로 결정을 내려야 하는 주인공의 이야기를 3인칭 내레이션으로 단편 소설을 쓰세요.</INST>안나는 여러 대학교의 입학 가능성이 있었으나, 제발스러운 직업들을 향해 유혹 당하는 경향이 있어 본인이 가진 진로와 관심사에 맞도록 철저히 검토한 결과, 건축학과를 준비하는 과정에 들어갑니다. 딱 이 시점에서, 안나는 과거의 잘못된 선택 때문에 확고한 결정을 내리기 매우 힘들다는 것에 고통을 겪게 됩니다. 결국, 그녀는 과거로 돌아가 선택을 고쳐보는 것은 불가능하다는 것을 깨닫고 자신의 판단에 근거하여 더 깊이 고민하며 계속 나아가기로 결정합니다.</s>',\n",
       "  '<s><INST>집의 3D 모델을 렌더링합니다.</INST>집의 3D 모델링 결과물 입니다. (이미지 첨부)</s>',\n",
       "  '<s><INST>이 문장에 철자와 문법 오류가 있는지 평가하세요.\\n\\n그는 식사를 마치고 식당을 나섰습니다.</INST>그 문장에는 철자나 문법에 대한 오류가 없습니다.</s>']}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "def split_data(dataset, valid_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    valid_indices = indices[:int(len(dataset) * valid_ratio)]\n",
    "    train_indices = indices[int(len(dataset) * valid_ratio):]\n",
    "    \n",
    "    return dataset.select(train_indices), dataset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data(pruned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 39696\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 9924\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['<s><INST>성공적인 사업 계획에 필요한 세 가지 기본 요소를 나열하십시오.</INST>사업 조사, 이윤과 예산 수립, 경제 분석</s>',\n",
       "  '<s><INST>미국 남성의 평균 키는 얼마입니까?</INST>1.77m</s>',\n",
       "  '<s><INST>다음 텍스트를 기술, 금융, 스포츠 또는 정치와 관련된 것으로 분류하세요.\\n\\n새로운 IPL 시즌은 뭄바이와 첸나이의 블록버스터 경기로 개막합니다.</INST>스포츠</s>']}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420b7d93fcab411ca2a80288fe63f2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "21117930"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/Bingsu/ko_alpaca_data/train.jsonl'\n",
    "\n",
    "train_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b07d7fc02d44fbb38f73dcbbc32699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5229310"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/Bingsu/ko_alpaca_data/valid.jsonl'\n",
    "\n",
    "valid_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Total parameters 227.308M\n",
      "Trainable parameters 0.051M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 1.813, Val took 34.017s\n",
      "Iter 10: Train loss 1.697, It/sec 0.603, Tokens/sec 465.439\n"
     ]
    }
   ],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Total parameters 228.076M\n",
      "Trainable parameters 0.819M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 1.806, Val took 47.262s\n",
      "Iter 10: Train loss 1.640, It/sec 0.279, Tokens/sec 215.259\n",
      "Iter 20: Train loss 1.662, It/sec 0.311, Tokens/sec 208.708\n",
      "Iter 30: Train loss 1.382, It/sec 0.279, Tokens/sec 170.474\n",
      "Iter 40: Train loss 1.350, It/sec 0.310, Tokens/sec 199.947\n",
      "Iter 50: Train loss 1.371, It/sec 0.269, Tokens/sec 190.877\n",
      "Iter 60: Train loss 1.359, It/sec 0.287, Tokens/sec 190.414\n",
      "Iter 70: Train loss 1.254, It/sec 0.268, Tokens/sec 192.671\n",
      "Iter 80: Train loss 1.251, It/sec 0.275, Tokens/sec 189.991\n",
      "Iter 90: Train loss 1.306, It/sec 0.275, Tokens/sec 197.304\n",
      "Iter 100: Train loss 1.160, It/sec 0.235, Tokens/sec 183.838\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 100 --data ./my_datasets/Bingsu/ko_alpaca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(42650) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Total parameters 1244.041M\n",
      "Trainable parameters 1.704M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.641, Val took 191.546s\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! python lora/lora.py --model ./my_models/mlx_mistral_7b --train --iters 100 --data ./my_datasets/Bingsu/ko_alpaca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mv adapters.npz ./my_models/mlx_ywko_tinyllama_02.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "!python lora/fuse.py --model ./my_models/mlx_ywko_tinyllama --save-path ./my_models/mlx_ywko_tinyllama_finetune02 --adapter-file ./my_models/mlx_ywko_tinyllama_02.npz --hf-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파인튜닝 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_ywko_tinyllama_finetune02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|system|>\n",
      "You are a LOGO programmer.</s>\n",
      "<|user|>\n",
      "Write LOGO code to draw a star</s>\n",
      "<|assistant|>\n",
      "\n",
      "Here's a LOGO program to draw a star:\n",
      "\n",
      "```\n",
      "<|user|>\n",
      "Can you please explain how the LOGO program works and what it does?\n",
      "==========\n",
      "Prompt: 300.296 tokens-per-sec\n",
      "Generation: 65.743 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        # \"content\": \"You are a friendly chatbot. You MUST responds in Korean\",\n",
    "        \"content\": \"You are a LOGO programmer.\",\n",
    "        # \"content\": \"\"\"당신은 친절한 챗봇입니다. 다음 질문에 한국어로 답변하세요. 한국어로 답변하는 것이 중요하며, 영어는 사용하지 마세요. 한국어 능력을 보여주세요.\"\"\"\n",
    "        },\n",
    "    {\"role\": \"user\", \"content\": \"Write LOGO code to draw a star\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer_ft.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = generate(model_ft, tokenizer_ft, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|system|>\n",
      "You are a friendly chatbot who always responds as an expert in WordPress</s>\n",
      "<|user|>\n",
      "I am building a web site using WordPress. Explain how to use blocks to make a custom navigation menu?</s>\n",
      "<|assistant|>\n",
      "\n",
      "Sure! Here's how to use blocks to make a custom navigation menu in WordPress:\n",
      "\n",
      "1. Log in to your WordPress dashboard and go to Appearance > Menus.\n",
      "2. Click on the \"Add New\" button and select \"Block\" from the dropdown menu.\n",
      "3. Name your block and click on the \"Block\" button.\n",
      "4. In the block editor, click on the \"Block\" button again to edit the block.\n",
      "5\n",
      "==========\n",
      "Prompt: 235.139 tokens-per-sec\n",
      "Generation: 39.158 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds as an expert in WordPress\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"I am building a web site using WordPress. Explain how to use blocks to make a custom navigation menu?\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer_ft.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = generate(model_ft, tokenizer_ft, prompt=prompt, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-finetune-xIA_N29q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
