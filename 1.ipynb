{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅 페이스 모델을 양자화 하고 Lora 로 한국어 학습 시키는 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q mlx_lm\n",
    "!pip install -q jinja2\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny Llama 1.1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[INFO] Loading\n",
      "Fetching 8 files:   0%|                                   | 0/8 [00:00<?, ?it/s]\n",
      "eval_results.json: 100%|███████████████████████| 566/566 [00:00<00:00, 2.08MB/s]\u001b[A\n",
      "Fetching 8 files: 100%|███████████████████████████| 8/8 [00:00<00:00, 15.94it/s]\n",
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/convert.py --hf-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 --mlx-path ./my_models/mlx_tinyllama -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_tinyllama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting?</s>\n",
      "<|assistant|>\n",
      "\n",
      "There is no definitive answer as to how many helicopters a human can eat in one sitting. However, it is generally accepted that humans are not designed to consume large quantities of food at once. A human's digestive system is designed to process food in small, digestible portions, which can take several hours to fully digest. Therefore, humans typically consume food in small, frequent meals throughout the day. In general, humans can consume up to a few\n",
      "==========\n",
      "Prompt: 176.528 tokens-per-sec\n",
      "Generation: 61.076 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|system|>\n",
      "당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요.</s>\n",
      "<|user|>\n",
      "가장 아름다운 꽃은 무엇인가요?</s>\n",
      "<|assistant|>\n",
      "\n",
      "가장 아름다운 꽃은 무었습니다. 왜 그런지 알려주세요.\n",
      "==========\n",
      "Prompt: 495.670 tokens-per-sec\n",
      "Generation: 60.501 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"가장 아름다운 꽃은 무엇인가요?\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47232b472cb74aaa95605103d9b6298f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.49M/8.49M [00:00<00:00, 10.8MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6f2533f4ad414e94ee1bd0b5f959a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/49620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Bingsu/ko_alpaca_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 49620\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3426ac1d4d9431fb498da999e38ad98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_instruction_output(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    output_text = example[\"output\"]\n",
    "    \n",
    "    if len(input_text) > 0:\n",
    "        return {'text': f\"<s><INST>{instruction}\\n\\n{input_text}</INST>{output_text}</s>\"}\n",
    "    else:\n",
    "        return {'text': f\"<s><INST>{instruction}</INST>{output_text}</s>\"}\n",
    "\n",
    "# 변환 함수 적용\n",
    "converted_dataset = dataset['train'].map(convert_to_instruction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['대기 오염을 어떻게 줄일 수 있나요?',\n",
       "  '어려운 결정을 내려야 했던 때를 설명하세요.',\n",
       "  '홀수 중 하나를 밝히세요.',\n",
       "  '다음 분수가 1/4에 해당하는 이유를 설명하세요.'],\n",
       " 'input': ['', '', '트위터, 인스타그램, 텔레그램', '4/16'],\n",
       " 'output': ['대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다.',\n",
       "  '제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다. 다양한 선택지가 있었지만 나의 관심사와 비전에 가장 부합하는 경력을 뽑는데 어려움이 있었습니다. 이 때 여러 사람들의 조언과 현재 상황에 대한 깊은 고민 끝에 적절한 선택을 내리게 되었습니다.',\n",
       "  '텔레그램입니다.',\n",
       "  '분모와 분자를 모두 4로 나눌 수 있기 때문에 1/4에 해당합니다.'],\n",
       " 'text': ['<s><INST>대기 오염을 어떻게 줄일 수 있나요?</INST>대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다.</s>',\n",
       "  '<s><INST>어려운 결정을 내려야 했던 때를 설명하세요.</INST>제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다. 다양한 선택지가 있었지만 나의 관심사와 비전에 가장 부합하는 경력을 뽑는데 어려움이 있었습니다. 이 때 여러 사람들의 조언과 현재 상황에 대한 깊은 고민 끝에 적절한 선택을 내리게 되었습니다.</s>',\n",
       "  '<s><INST>홀수 중 하나를 밝히세요.\\n\\n트위터, 인스타그램, 텔레그램</INST>텔레그램입니다.</s>',\n",
       "  '<s><INST>다음 분수가 1/4에 해당하는 이유를 설명하세요.\\n\\n4/16</INST>분모와 분자를 모두 4로 나눌 수 있기 때문에 1/4에 해당합니다.</s>']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dataset[3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_features = ['text']\n",
    "remove_features = [feat for feat in dataset['train'].features if feat not in keep_features]\n",
    "pruned_dataset = converted_dataset.remove_columns(column_names=remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['<s><INST>대기 오염을 어떻게 줄일 수 있나요?</INST>대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다.</s>',\n",
       "  '<s><INST>어려운 결정을 내려야 했던 때를 설명하세요.</INST>제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다. 다양한 선택지가 있었지만 나의 관심사와 비전에 가장 부합하는 경력을 뽑는데 어려움이 있었습니다. 이 때 여러 사람들의 조언과 현재 상황에 대한 깊은 고민 끝에 적절한 선택을 내리게 되었습니다.</s>',\n",
       "  '<s><INST>홀수 중 하나를 밝히세요.\\n\\n트위터, 인스타그램, 텔레그램</INST>텔레그램입니다.</s>',\n",
       "  '<s><INST>다음 분수가 1/4에 해당하는 이유를 설명하세요.\\n\\n4/16</INST>분모와 분자를 모두 4로 나눌 수 있기 때문에 1/4에 해당합니다.</s>']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_dataset[3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data(dataset, valid_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    valid_indices = indices[:int(len(dataset) * valid_ratio)]\n",
    "    train_indices = indices[int(len(dataset) * valid_ratio):]\n",
    "    \n",
    "    return dataset.select(train_indices), dataset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data(pruned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 39696\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 9924\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a9a2d715f54dc18d0b75a0712087cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "21117930"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/Bingsu/ko_alpaca_data/train.jsonl'\n",
    "\n",
    "train_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c138cf24c0441f49bfa96374501f1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5229310"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'my_datasets/Bingsu/ko_alpaca_data/valid.jsonl'\n",
    "\n",
    "valid_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Total parameters 228.076M\n",
      "Trainable parameters 0.819M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 1.805, Val took 43.673s\n",
      "Iter 10: Train loss 1.636, It/sec 0.290, Tokens/sec 223.683\n",
      "Iter 20: Train loss 1.657, It/sec 0.322, Tokens/sec 216.484\n",
      "Iter 30: Train loss 1.376, It/sec 0.286, Tokens/sec 174.584\n",
      "Iter 40: Train loss 1.350, It/sec 0.318, Tokens/sec 204.820\n",
      "Iter 50: Train loss 1.371, It/sec 0.274, Tokens/sec 194.649\n",
      "Iter 60: Train loss 1.359, It/sec 0.302, Tokens/sec 200.184\n",
      "Iter 70: Train loss 1.253, It/sec 0.283, Tokens/sec 203.665\n",
      "Iter 80: Train loss 1.250, It/sec 0.283, Tokens/sec 195.227\n",
      "Iter 90: Train loss 1.307, It/sec 0.278, Tokens/sec 199.544\n",
      "Iter 100: Train loss 1.161, It/sec 0.276, Tokens/sec 215.837\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 1.234, It/sec 0.327, Tokens/sec 208.347\n",
      "Iter 120: Train loss 1.267, It/sec 0.275, Tokens/sec 186.982\n",
      "Iter 130: Train loss 1.240, It/sec 0.255, Tokens/sec 198.416\n",
      "Iter 140: Train loss 1.188, It/sec 0.307, Tokens/sec 217.881\n",
      "Iter 150: Train loss 1.240, It/sec 0.288, Tokens/sec 207.147\n",
      "Iter 160: Train loss 1.237, It/sec 0.329, Tokens/sec 210.603\n",
      "Iter 170: Train loss 1.297, It/sec 0.332, Tokens/sec 213.010\n",
      "Iter 180: Train loss 1.164, It/sec 0.329, Tokens/sec 218.558\n",
      "Iter 190: Train loss 1.274, It/sec 0.288, Tokens/sec 182.286\n",
      "Iter 200: Train loss 1.226, It/sec 0.347, Tokens/sec 220.250\n",
      "Iter 200: Val loss 1.202, Val took 43.257s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 1.241, It/sec 0.348, Tokens/sec 215.801\n",
      "Iter 220: Train loss 1.156, It/sec 0.306, Tokens/sec 210.121\n",
      "Iter 230: Train loss 1.206, It/sec 0.280, Tokens/sec 217.299\n",
      "Iter 240: Train loss 1.171, It/sec 0.299, Tokens/sec 213.087\n",
      "Iter 250: Train loss 1.149, It/sec 0.283, Tokens/sec 196.861\n",
      "Iter 260: Train loss 1.167, It/sec 0.282, Tokens/sec 203.426\n",
      "Iter 270: Train loss 1.193, It/sec 0.337, Tokens/sec 236.563\n",
      "Iter 280: Train loss 1.178, It/sec 0.277, Tokens/sec 209.166\n",
      "Iter 290: Train loss 1.274, It/sec 0.424, Tokens/sec 227.064\n",
      "Iter 300: Train loss 1.249, It/sec 0.336, Tokens/sec 219.305\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 1.130, It/sec 0.139, Tokens/sec 106.073\n",
      "Iter 320: Train loss 1.262, It/sec 0.269, Tokens/sec 188.633\n",
      "Iter 330: Train loss 1.200, It/sec 0.279, Tokens/sec 192.402\n",
      "Iter 340: Train loss 1.221, It/sec 0.313, Tokens/sec 220.187\n",
      "Iter 350: Train loss 1.218, It/sec 0.347, Tokens/sec 217.982\n",
      "Iter 360: Train loss 1.114, It/sec 0.259, Tokens/sec 186.973\n",
      "Iter 370: Train loss 1.108, It/sec 0.279, Tokens/sec 190.900\n",
      "Iter 380: Train loss 1.173, It/sec 0.293, Tokens/sec 192.781\n",
      "Iter 390: Train loss 1.209, It/sec 0.302, Tokens/sec 197.185\n",
      "Iter 400: Train loss 1.160, It/sec 0.298, Tokens/sec 233.749\n",
      "Iter 400: Val loss 1.173, Val took 43.522s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 1.248, It/sec 0.312, Tokens/sec 207.336\n",
      "Iter 420: Train loss 1.123, It/sec 0.311, Tokens/sec 195.995\n",
      "Iter 430: Train loss 1.183, It/sec 0.353, Tokens/sec 202.593\n",
      "Iter 440: Train loss 1.246, It/sec 0.307, Tokens/sec 191.396\n",
      "Iter 450: Train loss 1.154, It/sec 0.199, Tokens/sec 143.490\n",
      "Iter 460: Train loss 1.153, It/sec 0.264, Tokens/sec 182.006\n",
      "Iter 470: Train loss 1.239, It/sec 0.286, Tokens/sec 194.252\n",
      "Iter 480: Train loss 1.185, It/sec 0.243, Tokens/sec 189.636\n",
      "Iter 490: Train loss 1.249, It/sec 0.292, Tokens/sec 204.225\n",
      "Iter 500: Train loss 1.132, It/sec 0.272, Tokens/sec 206.091\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 1.103, It/sec 0.304, Tokens/sec 207.392\n",
      "Iter 520: Train loss 1.156, It/sec 0.249, Tokens/sec 183.525\n",
      "Iter 530: Train loss 1.148, It/sec 0.292, Tokens/sec 193.447\n",
      "Iter 540: Train loss 1.201, It/sec 0.316, Tokens/sec 222.682\n",
      "Iter 550: Train loss 1.249, It/sec 0.384, Tokens/sec 219.802\n",
      "Iter 560: Train loss 1.222, It/sec 0.331, Tokens/sec 213.336\n",
      "Iter 570: Train loss 1.188, It/sec 0.301, Tokens/sec 204.784\n",
      "Iter 580: Train loss 1.135, It/sec 0.242, Tokens/sec 170.922\n",
      "Iter 590: Train loss 1.143, It/sec 0.254, Tokens/sec 192.035\n",
      "Iter 600: Train loss 1.177, It/sec 0.335, Tokens/sec 211.460\n",
      "Iter 600: Val loss 1.160, Val took 43.333s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 1.166, It/sec 0.317, Tokens/sec 213.266\n",
      "Iter 620: Train loss 1.094, It/sec 0.261, Tokens/sec 187.314\n",
      "Iter 630: Train loss 1.204, It/sec 0.343, Tokens/sec 208.916\n",
      "Iter 640: Train loss 1.139, It/sec 0.283, Tokens/sec 207.396\n",
      "Iter 650: Train loss 1.226, It/sec 0.301, Tokens/sec 223.718\n",
      "Iter 660: Train loss 1.126, It/sec 0.314, Tokens/sec 220.736\n",
      "Iter 670: Train loss 1.100, It/sec 0.287, Tokens/sec 192.872\n",
      "Iter 680: Train loss 1.187, It/sec 0.319, Tokens/sec 206.145\n",
      "Iter 690: Train loss 1.185, It/sec 0.276, Tokens/sec 187.401\n",
      "Iter 700: Train loss 1.098, It/sec 0.283, Tokens/sec 213.291\n",
      "Iter 700: Saved adapter weights to adapters.npz.\n",
      "Iter 710: Train loss 1.183, It/sec 0.349, Tokens/sec 208.038\n",
      "Iter 720: Train loss 1.144, It/sec 0.119, Tokens/sec 92.065\n",
      "Iter 730: Train loss 1.146, It/sec 0.322, Tokens/sec 215.376\n",
      "Iter 740: Train loss 1.157, It/sec 0.275, Tokens/sec 206.249\n",
      "Iter 750: Train loss 1.182, It/sec 0.282, Tokens/sec 190.943\n",
      "Iter 760: Train loss 1.211, It/sec 0.284, Tokens/sec 199.687\n",
      "Iter 770: Train loss 1.176, It/sec 0.256, Tokens/sec 198.385\n",
      "Iter 780: Train loss 1.112, It/sec 0.216, Tokens/sec 165.578\n",
      "Iter 790: Train loss 1.276, It/sec 0.235, Tokens/sec 176.749\n",
      "Iter 800: Train loss 1.178, It/sec 0.274, Tokens/sec 182.017\n",
      "Iter 800: Val loss 1.152, Val took 43.120s\n",
      "Iter 800: Saved adapter weights to adapters.npz.\n",
      "Iter 810: Train loss 1.166, It/sec 0.259, Tokens/sec 197.538\n",
      "Iter 820: Train loss 1.111, It/sec 0.297, Tokens/sec 213.666\n",
      "Iter 830: Train loss 1.119, It/sec 0.335, Tokens/sec 213.883\n",
      "Iter 840: Train loss 1.166, It/sec 0.309, Tokens/sec 210.107\n",
      "Iter 850: Train loss 1.097, It/sec 0.291, Tokens/sec 221.235\n",
      "Iter 860: Train loss 1.090, It/sec 0.291, Tokens/sec 210.764\n",
      "Iter 870: Train loss 1.157, It/sec 0.305, Tokens/sec 185.004\n",
      "Iter 880: Train loss 1.214, It/sec 0.369, Tokens/sec 233.643\n",
      "Iter 890: Train loss 1.202, It/sec 0.296, Tokens/sec 200.473\n",
      "Iter 900: Train loss 1.099, It/sec 0.249, Tokens/sec 188.733\n",
      "Iter 900: Saved adapter weights to adapters.npz.\n",
      "Iter 910: Train loss 1.169, It/sec 0.289, Tokens/sec 187.050\n",
      "Iter 920: Train loss 1.090, It/sec 0.287, Tokens/sec 192.435\n",
      "Iter 930: Train loss 1.185, It/sec 0.242, Tokens/sec 179.301\n",
      "Iter 940: Train loss 1.137, It/sec 0.285, Tokens/sec 196.581\n",
      "Iter 950: Train loss 1.180, It/sec 0.353, Tokens/sec 219.793\n",
      "Iter 960: Train loss 1.185, It/sec 0.244, Tokens/sec 192.454\n",
      "Iter 970: Train loss 1.080, It/sec 0.324, Tokens/sec 201.233\n",
      "Iter 980: Train loss 1.100, It/sec 0.334, Tokens/sec 207.174\n",
      "Iter 990: Train loss 1.161, It/sec 0.348, Tokens/sec 199.841\n",
      "Iter 1000: Train loss 1.179, It/sec 0.340, Tokens/sec 203.921\n",
      "Iter 1000: Val loss 1.145, Val took 42.813s\n",
      "Iter 1000: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "# iteration 수와 layer 수를 적당히 작게 하면 (허접하지만) 빨리 튜닝 결과를 얻어볼 수 있습니다.\n",
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1\n",
    "#\n",
    "# 그리고 100 iteration 마다 중간 결과를 adapters.npz 에 저장하므로 별도로 노트북을 열어서 다음 셀 이후의 작업을 실행시키면 중간 학습 과정에서의 결과물을 미리 보실 수 있습니다. \n",
    "#\n",
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_tinyllama --train --data ./my_datasets/Bingsu/ko_alpaca_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: adapters.npz: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mv adapters.npz ./my_models/mlx_tinyllama_01.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_tinyllama --save-path ./my_models/mlx_tinyllama_ft_01 --adapter-file ./my_models/mlx_tinyllama_01.npz --hf-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파인튜닝 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_tinyllama_ft_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|system|>\n",
      "당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요.</s>\n",
      "<|user|>\n",
      "가장 아름다운 꽃은 무엇인가요?</s>\n",
      "<|assistant|>\n",
      "\n",
      "가장 아름다운 꽃은 매우 많은 것들이 있습니다. 예를 들어, 매우 많은 양념의 매우 많은 양념의 매우 많은 \n",
      "==========\n",
      "Prompt: 499.790 tokens-per-sec\n",
      "Generation: 58.987 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"가장 아름다운 꽃은 무엇인가요?\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer_ft.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = generate(model_ft, tokenizer_ft, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[INFO] Loading\n",
      "Fetching 11 files: 100%|█████████████████████| 11/11 [00:00<00:00, 86724.33it/s]\n",
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/convert.py --hf-path mistralai/Mistral-7B-Instruct-v0.2 --mlx-path ./my_models/mlx_mistral_7b -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_mistral_7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 가장 아름다운 꽃은 무엇인가요?\n",
      "그 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 맹 \n",
      "==========\n",
      "Prompt: 67.458 tokens-per-sec\n",
      "Generation: 16.508 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"가장 아름다운 꽃은 무엇인가요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 가장 아름다운 꽃은 무엇인가요?\n",
      "(What is the most beautiful flower?) is a common question that has been asked throughout history. The answer, of course, depends on personal preferences and cultural backgrounds. However, there are some flowers that are universally recognized for their beauty and symbolism. Here are ten of the most beautiful flowers in the world:\n",
      "\n",
      "1. Rose: The rose is often considered the queen of flowers due to its timeless beauty and symbolism. With their delicate petals, sweet fragrance,\n",
      "==========\n",
      "Prompt: 0.909 tokens-per-sec\n",
      "Generation: 0.476 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"가장 아름다운 꽃은 무엇인가요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 가장 아름다운 꽃은 무엇인가요?\n",
      "\n",
      "\n",
      "안녕하세요! 가장 아름다운 꽃은 장미입니다. 장미는 멸측되지 않고 지속적으로 피어나는 꽃으로, 멸측되지 않는 아름다움과 지속적인 존재감에서 느\n",
      "==========\n",
      "Prompt: 3.235 tokens-per-sec\n",
      "Generation: 0.941 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 가장 아름다운 꽃은 무엇인가요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 건강을 지키는 비결은 무엇인가?\n",
      "\n",
      "\n",
      "안녕하세요! 건강을 지키는 비결에는 많은 것이 있지만, 일상적인 생활에서 가장 중요한 것들을 말하겠습니다.\n",
      "\n",
      "1. 건강한 식단 섭취: 과일, 채소, 견고한 단백질\n",
      "==========\n",
      "Prompt: 76.045 tokens-per-sec\n",
      "Generation: 9.859 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 건강을 지키는 비결은 무엇인가?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 주요 프로그래밍 언어는 어떤 것이 있나요?\n",
      "\n",
      "\n",
      "대부분의 프로그래밍 언어가 있습니다. C, Java, Python, JavaScript, Ruby, Swift 등이 있습니다. 이 중에서도 한국어로 프로그래밍 언어를 찾으시면 다음과 같습니다.\n",
      "\n",
      "1. 코틀린 (Kotlin\n",
      "==========\n",
      "Prompt: 70.124 tokens-per-sec\n",
      "Generation: 8.449 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 주요 프로그래밍 언어는 어떤 것이 있나요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Total parameters 1244.041M\n",
      "Trainable parameters 1.704M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.639, Val took 191.334s\n",
      "Iter 10: Train loss 2.122, It/sec 0.071, Tokens/sec 41.529\n",
      "Iter 20: Train loss 1.788, It/sec 0.082, Tokens/sec 42.068\n",
      "Iter 30: Train loss 1.549, It/sec 0.077, Tokens/sec 35.038\n",
      "Iter 40: Train loss 1.502, It/sec 0.080, Tokens/sec 39.123\n",
      "Iter 50: Train loss 1.459, It/sec 0.068, Tokens/sec 37.114\n",
      "Iter 60: Train loss 1.489, It/sec 0.073, Tokens/sec 36.748\n",
      "Iter 70: Train loss 1.402, It/sec 0.076, Tokens/sec 40.875\n",
      "Iter 80: Train loss 1.398, It/sec 0.076, Tokens/sec 38.947\n",
      "Iter 90: Train loss 1.388, It/sec 0.076, Tokens/sec 41.874\n",
      "Iter 100: Train loss 1.272, It/sec 0.069, Tokens/sec 40.874\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 1.358, It/sec 0.088, Tokens/sec 42.956\n",
      "Iter 120: Train loss 1.397, It/sec 0.072, Tokens/sec 36.975\n",
      "Iter 130: Train loss 1.368, It/sec 0.067, Tokens/sec 39.529\n",
      "Iter 140: Train loss 1.305, It/sec 0.082, Tokens/sec 43.952\n",
      "Iter 150: Train loss 1.359, It/sec 0.072, Tokens/sec 39.609\n",
      "Iter 160: Train loss 1.356, It/sec 0.083, Tokens/sec 40.618\n",
      "Iter 170: Train loss 1.400, It/sec 0.084, Tokens/sec 40.902\n",
      "Iter 180: Train loss 1.303, It/sec 0.084, Tokens/sec 42.149\n",
      "Iter 190: Train loss 1.404, It/sec 0.060, Tokens/sec 29.332\n",
      "Iter 200: Train loss 1.356, It/sec 0.088, Tokens/sec 42.820\n",
      "Iter 200: Val loss 1.327, Val took 189.971s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 1.440, It/sec 0.097, Tokens/sec 45.364\n",
      "Iter 220: Train loss 1.225, It/sec 0.076, Tokens/sec 39.876\n",
      "Iter 230: Train loss 1.283, It/sec 0.073, Tokens/sec 43.349\n",
      "Iter 240: Train loss 1.319, It/sec 0.082, Tokens/sec 43.531\n",
      "Iter 250: Train loss 1.293, It/sec 0.080, Tokens/sec 41.815\n",
      "Iter 260: Train loss 1.325, It/sec 0.071, Tokens/sec 38.657\n",
      "Iter 270: Train loss 1.324, It/sec 0.093, Tokens/sec 49.330\n",
      "Iter 280: Train loss 1.278, It/sec 0.072, Tokens/sec 41.951\n",
      "Iter 290: Train loss 1.384, It/sec 0.111, Tokens/sec 46.716\n",
      "Iter 300: Train loss 1.378, It/sec 0.091, Tokens/sec 45.144\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 1.259, It/sec 0.047, Tokens/sec 27.178\n",
      "Iter 320: Train loss 1.362, It/sec 0.067, Tokens/sec 35.935\n",
      "Iter 330: Train loss 1.335, It/sec 0.074, Tokens/sec 38.715\n",
      "Iter 340: Train loss 1.343, It/sec 0.083, Tokens/sec 43.840\n",
      "Iter 350: Train loss 1.378, It/sec 0.088, Tokens/sec 42.621\n",
      "Iter 360: Train loss 1.244, It/sec 0.065, Tokens/sec 35.667\n",
      "Iter 370: Train loss 1.231, It/sec 0.073, Tokens/sec 37.434\n",
      "Iter 380: Train loss 1.300, It/sec 0.073, Tokens/sec 37.308\n",
      "Iter 390: Train loss 1.338, It/sec 0.081, Tokens/sec 39.538\n",
      "Iter 400: Train loss 1.280, It/sec 0.078, Tokens/sec 46.006\n",
      "Iter 400: Val loss 1.295, Val took 185.604s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 1.390, It/sec 0.075, Tokens/sec 37.773\n",
      "Iter 420: Train loss 1.274, It/sec 0.035, Tokens/sec 17.129\n",
      "Iter 430: Train loss 1.341, It/sec 0.033, Tokens/sec 14.143\n",
      "Iter 440: Train loss 1.368, It/sec 0.034, Tokens/sec 16.277\n",
      "Iter 450: Train loss 1.268, It/sec 0.054, Tokens/sec 29.368\n",
      "Iter 460: Train loss 1.287, It/sec 0.068, Tokens/sec 34.719\n",
      "Iter 470: Train loss 1.354, It/sec 0.056, Tokens/sec 29.264\n",
      "Iter 480: Train loss 1.294, It/sec 0.036, Tokens/sec 21.397\n",
      "Iter 490: Train loss 1.327, It/sec 0.065, Tokens/sec 35.446\n",
      "Iter 500: Train loss 1.222, It/sec 0.030, Tokens/sec 17.689\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 1.198, It/sec 0.028, Tokens/sec 14.347\n",
      "Iter 520: Train loss 1.241, It/sec 0.036, Tokens/sec 20.183\n",
      "Iter 530: Train loss 1.243, It/sec 0.063, Tokens/sec 31.524\n",
      "Iter 540: Train loss 1.311, It/sec 0.076, Tokens/sec 40.596\n",
      "Iter 550: Train loss 1.367, It/sec 0.093, Tokens/sec 40.999\n",
      "Iter 560: Train loss 1.317, It/sec 0.079, Tokens/sec 39.185\n",
      "Iter 570: Train loss 1.300, It/sec 0.075, Tokens/sec 38.830\n",
      "Iter 580: Train loss 1.272, It/sec 0.066, Tokens/sec 34.512\n",
      "Iter 590: Train loss 1.292, It/sec 0.065, Tokens/sec 36.216\n",
      "Iter 600: Train loss 1.252, It/sec 0.081, Tokens/sec 38.722\n",
      "Iter 600: Val loss 1.267, Val took 192.237s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 1.309, It/sec 0.078, Tokens/sec 39.633\n",
      "Iter 620: Train loss 1.192, It/sec 0.064, Tokens/sec 34.382\n",
      "Iter 630: Train loss 1.316, It/sec 0.082, Tokens/sec 38.337\n",
      "Iter 640: Train loss 1.224, It/sec 0.070, Tokens/sec 38.668\n",
      "Iter 650: Train loss 1.325, It/sec 0.073, Tokens/sec 41.730\n",
      "Iter 660: Train loss 1.252, It/sec 0.080, Tokens/sec 42.011\n",
      "Iter 670: Train loss 1.202, It/sec 0.071, Tokens/sec 35.936\n",
      "Iter 680: Train loss 1.253, It/sec 0.071, Tokens/sec 35.292\n",
      "Iter 690: Train loss 1.245, It/sec 0.063, Tokens/sec 32.568\n",
      "Iter 700: Train loss 1.176, It/sec 0.072, Tokens/sec 40.462\n",
      "Iter 700: Saved adapter weights to adapters.npz.\n",
      "Iter 710: Train loss 1.267, It/sec 0.087, Tokens/sec 39.523\n",
      "Iter 720: Train loss 1.190, It/sec 0.042, Tokens/sec 24.955\n",
      "Iter 730: Train loss 1.205, It/sec 0.079, Tokens/sec 39.973\n",
      "Iter 740: Train loss 1.203, It/sec 0.065, Tokens/sec 36.946\n",
      "Iter 750: Train loss 1.221, It/sec 0.063, Tokens/sec 33.305\n",
      "Iter 760: Train loss 1.266, It/sec 0.067, Tokens/sec 36.018\n",
      "Iter 770: Train loss 1.253, It/sec 0.061, Tokens/sec 35.749\n",
      "Iter 780: Train loss 1.166, It/sec 0.058, Tokens/sec 33.257\n",
      "Iter 790: Train loss 1.293, It/sec 0.056, Tokens/sec 32.398\n",
      "Iter 800: Train loss 1.200, It/sec 0.063, Tokens/sec 32.478\n",
      "Iter 800: Val loss 1.186, Val took 219.358s\n",
      "Iter 800: Saved adapter weights to adapters.npz.\n",
      "Iter 810: Train loss 1.192, It/sec 0.049, Tokens/sec 28.280\n",
      "Iter 820: Train loss 1.151, It/sec 0.075, Tokens/sec 40.837\n",
      "Iter 830: Train loss 1.150, It/sec 0.093, Tokens/sec 44.846\n",
      "Iter 840: Train loss 1.210, It/sec 0.078, Tokens/sec 40.322\n",
      "Iter 850: Train loss 1.146, It/sec 0.080, Tokens/sec 45.513\n",
      "Iter 860: Train loss 1.094, It/sec 0.073, Tokens/sec 40.459\n",
      "Iter 870: Train loss 1.176, It/sec 0.082, Tokens/sec 37.902\n",
      "Iter 880: Train loss 1.259, It/sec 0.093, Tokens/sec 44.536\n",
      "Iter 890: Train loss 1.197, It/sec 0.077, Tokens/sec 40.338\n",
      "Iter 900: Train loss 1.132, It/sec 0.065, Tokens/sec 37.000\n",
      "Iter 900: Saved adapter weights to adapters.npz.\n",
      "Iter 910: Train loss 1.229, It/sec 0.077, Tokens/sec 37.359\n",
      "Iter 920: Train loss 1.124, It/sec 0.075, Tokens/sec 37.487\n",
      "Iter 930: Train loss 1.172, It/sec 0.062, Tokens/sec 35.178\n",
      "Iter 940: Train loss 1.184, It/sec 0.079, Tokens/sec 40.975\n",
      "Iter 950: Train loss 1.149, It/sec 0.059, Tokens/sec 27.965\n",
      "Iter 960: Train loss 1.207, It/sec 0.059, Tokens/sec 35.292\n",
      "Iter 970: Train loss 1.116, It/sec 0.088, Tokens/sec 41.294\n",
      "Iter 980: Train loss 1.113, It/sec 0.091, Tokens/sec 42.574\n",
      "Iter 990: Train loss 1.153, It/sec 0.090, Tokens/sec 39.394\n",
      "Iter 1000: Train loss 1.168, It/sec 0.082, Tokens/sec 37.654\n",
      "Iter 1000: Val loss 1.154, Val took 193.389s\n",
      "Iter 1000: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_mistral_7b --train --data ./my_datasets/Bingsu/ko_alpaca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(43895) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mv adapters.npz ./my_models/mlx_mistral_01.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(43896) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_mistral_7b --save-path ./my_models/mlx_mistral_7b_ft_01 --adapter-file ./my_models/mlx_mistral_01.npz --hf-path mistralai/Mistral-7B-Instruct-v0.2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파인튜닝 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_mistral_7b_ft_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 가장 아름다운 꽃은 무엇인가요?\n",
      "\n",
      "==========\n",
      "No tokens generated for this prompt\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"당신은 친절한 챗봇입니다. 항상 한국어로만 답을 하세요. 가장 아름다운 꽃은 무엇인가요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 가장 아름다운 꽃은 무엇인가요?\n",
      "\n",
      "\n",
      "가장 아름다운 꽃은 빛나는 황색 꽃입니다.\n",
      "==========\n",
      "Prompt: 66.969 tokens-per-sec\n",
      "Generation: 17.098 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"가장 아름다운 꽃은 무엇인가요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 건강을 지키는 비결은 무엇인가?\n",
      "\n",
      "\n",
      "건강을 지키는 비결은 건강한 식단, 충분한 수면, 적절한 운동, 스트레스 관리 등이 있습니다.\n",
      "==========\n",
      "Prompt: 66.138 tokens-per-sec\n",
      "Generation: 17.030 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"건강을 지키는 비결은 무엇인가?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 주요 프로그래밍 언어는 어떤 것이 있나요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Python, Java, C++, Ruby, JavaScript, Swift, Go, R, PHP, C#\n",
      "\n",
      "이 프로그래밍 언어들은 모두 다른 방식으로 프로그래밍을 할 수 있습니다.\n",
      "==========\n",
      "Prompt: 89.954 tokens-per-sec\n",
      "Generation: 16.641 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"주요 프로그래밍 언어는 어떤 것이 있나요?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-finetune-xIA_N29q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
