{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅 페이스에서 StarCoder 를 가져와서 기능을 테스트 해보는 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q mlx_lm\n",
    "!pip install -q jinja2\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/g0/wv8n5z_j2gvb6v20whbkkvdm0000gn/T/pip-req-build-vs_d57v4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/g0/wv8n5z_j2gvb6v20whbkkvdm0000gn/T/pip-req-build-vs_d57v4\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 00c1d87a7d5c8dfb4554370983b5a3f7c069edd7\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/goyang-u/.local/share/virtualenvs/mlx_exercise-9jLeNJlb/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8714194 sha256=01af7bf87829a84e5279fc90936f4583f70a3440094598ab7cf59c57596937f2\n",
      "  Stored in directory: /private/var/folders/g0/wv8n5z_j2gvb6v20whbkkvdm0000gn/T/pip-ephem-wheel-cache-_p60pgix/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "Successfully installed transformers-4.39.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- -----------\n",
      "aiohttp                   3.9.3\n",
      "aiosignal                 1.3.1\n",
      "appnope                   0.1.4\n",
      "asttokens                 2.4.1\n",
      "attrs                     23.2.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.2.2\n",
      "charset-normalizer        3.3.2\n",
      "comm                      0.2.1\n",
      "datasets                  2.18.0\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.8\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "filelock                  3.13.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2024.2.0\n",
      "huggingface-hub           0.21.4\n",
      "idna                      3.6\n",
      "ipykernel                 6.29.3\n",
      "ipython                   8.22.2\n",
      "ipywidgets                8.1.2\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.0\n",
      "jupyter_core              5.7.1\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_widgets        3.0.10\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "mlx                       0.6.0\n",
      "mlx-lm                    0.1.0\n",
      "multidict                 6.0.5\n",
      "multiprocess              0.70.16\n",
      "nbclient                  0.9.0\n",
      "nbconvert                 7.16.2\n",
      "nbformat                  5.9.2\n",
      "nest-asyncio              1.6.0\n",
      "numpy                     1.26.4\n",
      "packaging                 23.2\n",
      "pandas                    2.2.1\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pexpect                   4.9.0\n",
      "pip                       24.0\n",
      "platformdirs              4.2.0\n",
      "prompt-toolkit            3.0.43\n",
      "protobuf                  4.25.3\n",
      "psutil                    5.9.8\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   15.0.1\n",
      "pyarrow-hotfix            0.6\n",
      "Pygments                  2.17.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-dotenv             1.0.1\n",
      "pytz                      2024.1\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "referencing               0.33.0\n",
      "regex                     2023.12.25\n",
      "requests                  2.31.0\n",
      "rpds-py                   0.18.0\n",
      "safetensors               0.4.2\n",
      "setuptools                69.0.3\n",
      "six                       1.16.0\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "tinycss2                  1.2.1\n",
      "tokenizers                0.15.2\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.2\n",
      "traitlets                 5.14.1\n",
      "transformers              4.39.0.dev0\n",
      "typing_extensions         4.10.0\n",
      "tzdata                    2024.1\n",
      "urllib3                   2.2.1\n",
      "wcwidth                   0.2.13\n",
      "webencodings              0.5.1\n",
      "wheel                     0.42.0\n",
      "widgetsnbextension        4.0.10\n",
      "xxhash                    3.4.1\n",
      "yarl                      1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7a531c0ce748f0b2ff1a5d32ff54c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc085de674a43748dad9d05913bd475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb5640f0991462681e6e082f7224520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc5a34eeb83489e99e04c33629bea9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07df0e5f21764b01ba2711275e614071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c782ee43f244a0188d2222ad021114d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4b11c824e04ca4911b2aae4ac3287a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4e77533399446db7443d9d78e72631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: def print_hello_world():\n",
      "\n",
      "    print(\"Hello World!\")\n",
      "\n",
      "print_hello_world()\n",
      "\n",
      "# +\n",
      "# 1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2\n",
      "==========\n",
      "Prompt: 7.993 tokens-per-sec\n",
      "Generation: 15.647 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/starcoder2-7b-4bit\")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=\"def print_hello_world():\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlx_lm import convert\n",
    "# help(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러가 나고 안되네...\n",
    "# upload_repo = \"mlx_bigcode_starcoder2-7b-4bit\"\n",
    "# convert(\"bigcode/starcoder2-7b\", quantize=False, upload_repo=upload_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[INFO] Loading\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 57344.00it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/goyang-u/work/4x4/mlx_exercise/./mlx-examples/lora/convert.py\", line 91, in <module>\n",
      "    weights, config, tokenizer = utils.fetch_from_hub(args.hf_path)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/goyang-u/work/4x4/mlx_exercise/mlx-examples/lora/utils.py\", line 53, in fetch_from_hub\n",
      "    raise FileNotFoundError(\"No safetensors found in {}\".format(model_path))\n",
      "FileNotFoundError: No safetensors found in /Users/goyang-u/.cache/huggingface/hub/models--bigcode--starcoder/snapshots/b1af7f63dfbe5f2989b33399f1b99b58ff80a7d4\n"
     ]
    }
   ],
   "source": [
    "# !python ./mlx-examples/lora/convert.py --hf-path bigcode/starcoder --mlx-path ./my_models/mlx_starcoder -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_phi2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습용 데이터셋 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPL Programming Language 학습용 데이터셋을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('chrishayuk/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    q, a = example['text'].replace('<s>[INST]', '').replace('</s>', '').split('[/INST]')\n",
    "    return {'text': f\"### Question: {q}\\n ### Answer: {a}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset = dataset['train'].map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data(dataset, valid_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    valid_indices = indices[:int(len(dataset) * valid_ratio)]\n",
    "    train_indices = indices[int(len(dataset) * valid_ratio):]\n",
    "    \n",
    "    return dataset.select(train_indices), dataset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data(converted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/train.jsonl'\n",
    "\n",
    "train_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'my_datasets/chrishayuk/test/valid.jsonl'\n",
    "\n",
    "valid_data.to_json(output_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1\n",
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_phi2 --train --data ./my_datasets/chrishayuk/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv adapters.npz ./my_models/mlx_phi2_01.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_phi2 --save-path ./my_models/mlx_phi2_ft_01 --adapter-file ./my_models/mlx_phi2_01.npz --hf-path microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파인튜닝 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_phi2_ft_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.py 에서 이미 다운로드 하여 양자화 했다고 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./my_models/mlx_mistral_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python lora/lora.py --model ./my_models/mlx_ywko_tinyllama --train --iters 10 --data ./my_datasets/Bingsu/ko_alpaca_data --lora-layer 1\n",
    "! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_mistral_7b --train --data ./my_datasets/chrishayuk/test\n",
    "# ! python ./mlx-examples/lora/lora.py --model ./my_models/mlx_mistral_7b --train --iters 10 --data ./my_datasets/chrishayuk/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp adapters.npz ./my_models/mlx_mistral_7b_02.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./mlx-examples/lora/fuse.py --model ./my_models/mlx_mistral_7b --save-path ./my_models/mlx_mistral_7b_ft_02 --adapter-file ./my_models/mlx_mistral_7b_02.npz --hf-path mistralai/Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파인튜닝 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_ft, tokenizer_ft = load(\"./my_models/mlx_mistral_7b_ft_02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments only in the python programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using comments in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"write me a joke using remarks in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"Write a program that checks if a given year, is a leap year in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model_ft, tokenizer_ft, prompt=\"Write a program that prints hello world in the OPL programming language\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-finetune-xIA_N29q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
